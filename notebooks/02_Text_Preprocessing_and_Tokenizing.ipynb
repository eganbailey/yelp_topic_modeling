{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "In this notebook, we process and tokenize each review text. Tokenization transforms each review into a list of relevant tokens; these reviews are added to a corpus as a list. Since restaurants dominate the Yelp review space, two corpuses are developed: one for restaurants, and one for all other types of businesses.\n",
    "\n",
    "## Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "import textacy\n",
    "import pickle\n",
    "import time\n",
    "import string\n",
    "import unidecode\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "engine = create_engine('postgres://postgres:moop@52.25.218.143:5432/capstone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data from server\n",
    "\n",
    "### Query for Restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3221418, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = '''SELECT reviews.text as text,\n",
    "reviews.useful as useful,\n",
    "business.categories as categories\n",
    "FROM reviews\n",
    "INNER JOIN business ON reviews.business_id = business.business_id\n",
    "WHERE categories LIKE '%%Restaurants%%'\n",
    "'''\n",
    "\n",
    "restaurants = pd.read_sql_query(query, engine)\n",
    "restaurants.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query for Other Businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2040250, 5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = '''\n",
    "SELECT \n",
    "reviews.text as text, \n",
    "reviews.useful as useful, \n",
    "reviews.funny as funny, \n",
    "reviews.cool as cool,\n",
    "business.categories as categories\n",
    "FROM reviews\n",
    "INNER JOIN business ON reviews.business_id = business.business_id\n",
    "WHERE categories NOT LIKE '%%Restaurants%%'\n",
    "'''\n",
    "\n",
    "businesses = pd.read_sql_query(query, engine)\n",
    "businesses.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the text\n",
    "\n",
    "I use textacy's preprocess method to convert all the text to lowercase and remove numbers, URLs, and punctuation and save it to a file so we can load it in to work on.\n",
    "\n",
    "### Restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants.drop('categories', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24min 49s, sys: 0 ns, total: 24min 49s\n",
      "Wall time: 24min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "restaurants['processed'] = restaurants['text'].map(lambda x: textacy.preprocess.preprocess_text(x, lowercase=True, no_urls=True, no_punct=True, no_numbers=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants.drop('text', 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants.to_csv('../data/rests.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "businesses.drop('categories', 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16min 38s, sys: 0 ns, total: 16min 38s\n",
      "Wall time: 16min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "businesses['processed'] = businesses['text'].map(lambda x: textacy.preprocess.preprocess_text(x, lowercase=True, no_urls=True, no_punct=True, no_numbers=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "businesses.drop('text', 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "businesses.to_csv('../data/businesses.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing non-English reviews\n",
    "\n",
    "Since Yelp is used worldwide, we should check to see if any reviews were written using non-English characters as non-English words may not be tokenized or processed correctly. I define the function `isEnglish` to filter out non-ASCII characters as non-ASCII characters would predominantly be used by users typing reviews in non-English languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode('ascii')\n",
    "    except UnicodeEncodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants = pd.read_csv('../data/rests.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants.drop('Unnamed: 0', 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the boolean column `isEnglish` and then drop the columns that return True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants['isEnglish'] = restaurants['processed'].astype('unicode').apply(lambda x: isEnglish(x) == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants = restaurants[restaurants['isEnglish'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants.to_csv('../data/rests_english.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "businesses = pd.read_csv('../data/businesses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>cycle pub las vegas was a blast got a groupon ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>i thought tidy s flowers had a great reputatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>i too have been trying to book an appt to use ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>great place to bring dogs it s really a dog pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>decided to give this place a try since i was i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   useful  funny  cool                                          processed\n",
       "0       1      0     0  cycle pub las vegas was a blast got a groupon ...\n",
       "1       9      0     1  i thought tidy s flowers had a great reputatio...\n",
       "2       0      0     0  i too have been trying to book an appt to use ...\n",
       "3       0      0     0  great place to bring dogs it s really a dog pl...\n",
       "4       0      0     0  decided to give this place a try since i was i..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "businesses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "businesses['isEnglish'] = businesses['processed'].astype('str').astype('unicode').apply(lambda x: isEnglish(x) == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "businesses = businesses[businesses['isEnglish'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "businesses.to_csv('../data/businesses_english.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing\n",
    "\n",
    "In this section I set up a spacy tokenizer. We disable part-of-speech tagging, semantic parsing, and text categorization to reduce overall memory usage. We also create a filter function to eliminate stopwords and short tokens (less than 4 characters) from our final tokenized documents. The tokenized documents are then added to a list which we can pass through a vectorizer (see Notebook 3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = textacy.load_spacy(\"en_core_web_sm\", disable = (\"tagger\", \"parser\", \"ner\", \"textcat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_filter(token): #remove stopwords and tokens 4 char or less\n",
    "    return not (token.is_stop | len(token.text) <= 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants = pd.read_csv('../data/rests_english.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = restaurants['processed'].astype('str').astype('unicode').tolist() # The spacy pipeline requires raw text to be in utf-8 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized 10000 documents in 0.5264656027158101 minutes\n",
      "Tokenized 20000 documents in 0.9942031065622966 minutes\n",
      "Tokenized 30000 documents in 1.4545220096906026 minutes\n",
      "Tokenized 40000 documents in 1.927197806040446 minutes\n",
      "Tokenized 50000 documents in 2.3963774919509886 minutes\n",
      "Tokenized 60000 documents in 2.88506498336792 minutes\n",
      "Tokenized 70000 documents in 3.4080612897872924 minutes\n",
      "Tokenized 80000 documents in 3.921268590291341 minutes\n",
      "Tokenized 90000 documents in 4.4100446422894795 minutes\n",
      "Tokenized 100000 documents in 4.8793296098709105 minutes\n",
      "Tokenized 110000 documents in 5.344931999842326 minutes\n",
      "Tokenized 120000 documents in 5.839535077412923 minutes\n",
      "Tokenized 130000 documents in 6.326991760730744 minutes\n",
      "Tokenized 140000 documents in 6.799816346168518 minutes\n",
      "Tokenized 150000 documents in 7.271938117345174 minutes\n",
      "Tokenized 160000 documents in 7.764986379941305 minutes\n",
      "Tokenized 170000 documents in 8.238894959290823 minutes\n",
      "Tokenized 180000 documents in 8.718480745951334 minutes\n",
      "Tokenized 190000 documents in 9.215034635861715 minutes\n",
      "Tokenized 200000 documents in 9.702667792638143 minutes\n",
      "Tokenized 210000 documents in 10.167164599895477 minutes\n",
      "Tokenized 220000 documents in 10.64756980339686 minutes\n",
      "Tokenized 230000 documents in 11.129495179653167 minutes\n",
      "Tokenized 240000 documents in 11.607551829020183 minutes\n",
      "Tokenized 250000 documents in 12.104382892449697 minutes\n",
      "Tokenized 260000 documents in 12.587463553746542 minutes\n",
      "Tokenized 270000 documents in 13.057388416926067 minutes\n",
      "Tokenized 280000 documents in 13.520631392796835 minutes\n",
      "Tokenized 290000 documents in 13.989426763852437 minutes\n",
      "Tokenized 300000 documents in 14.455571548144023 minutes\n",
      "Tokenized 310000 documents in 14.919533566633861 minutes\n",
      "Tokenized 320000 documents in 15.368904157479603 minutes\n",
      "Tokenized 330000 documents in 15.83727988799413 minutes\n",
      "Tokenized 340000 documents in 16.30710616906484 minutes\n",
      "Tokenized 350000 documents in 16.771470177173615 minutes\n",
      "Tokenized 360000 documents in 17.27558926343918 minutes\n",
      "Tokenized 370000 documents in 17.753034027417502 minutes\n",
      "Tokenized 380000 documents in 18.244916880130766 minutes\n",
      "Tokenized 390000 documents in 18.718693081537882 minutes\n",
      "Tokenized 400000 documents in 19.19090830485026 minutes\n",
      "Tokenized 410000 documents in 19.65072455803553 minutes\n",
      "Tokenized 420000 documents in 20.130442972977956 minutes\n",
      "Tokenized 430000 documents in 20.611063480377197 minutes\n",
      "Tokenized 440000 documents in 21.104571469624837 minutes\n",
      "Tokenized 450000 documents in 21.596781448523203 minutes\n",
      "Tokenized 460000 documents in 22.07409901221593 minutes\n",
      "Tokenized 470000 documents in 22.55396680434545 minutes\n",
      "Tokenized 480000 documents in 23.042322758833567 minutes\n",
      "Tokenized 490000 documents in 23.534756437937418 minutes\n",
      "Tokenized 500000 documents in 24.0358646829923 minutes\n",
      "Tokenized 510000 documents in 24.53868341445923 minutes\n",
      "Tokenized 520000 documents in 25.021074577172598 minutes\n",
      "Tokenized 530000 documents in 25.525697151819866 minutes\n",
      "Tokenized 540000 documents in 26.003991357485454 minutes\n",
      "Tokenized 550000 documents in 26.473274660110473 minutes\n",
      "Tokenized 560000 documents in 26.965146124362946 minutes\n",
      "Tokenized 570000 documents in 27.433895969390868 minutes\n",
      "Tokenized 580000 documents in 27.91630522410075 minutes\n",
      "Tokenized 590000 documents in 28.407240947087605 minutes\n",
      "Tokenized 600000 documents in 28.874811482429504 minutes\n",
      "Tokenized 610000 documents in 29.36103809674581 minutes\n",
      "Tokenized 620000 documents in 29.871935804684956 minutes\n",
      "Tokenized 630000 documents in 30.3709561308225 minutes\n",
      "Tokenized 640000 documents in 30.86591006120046 minutes\n",
      "Tokenized 650000 documents in 31.352049855391183 minutes\n",
      "Tokenized 660000 documents in 31.843847167491912 minutes\n",
      "Tokenized 670000 documents in 32.330776528517404 minutes\n",
      "Tokenized 680000 documents in 32.7969820300738 minutes\n",
      "Tokenized 690000 documents in 33.281876333554585 minutes\n",
      "Tokenized 700000 documents in 33.773833040396376 minutes\n",
      "Tokenized 710000 documents in 34.26779896815618 minutes\n",
      "Tokenized 720000 documents in 34.761791976292926 minutes\n",
      "Tokenized 730000 documents in 35.25921460390091 minutes\n",
      "Tokenized 740000 documents in 35.7418868581454 minutes\n",
      "Tokenized 750000 documents in 36.239046271642046 minutes\n",
      "Tokenized 760000 documents in 36.73112113078435 minutes\n",
      "Tokenized 770000 documents in 37.215462148189545 minutes\n",
      "Tokenized 780000 documents in 37.69803107976914 minutes\n",
      "Tokenized 790000 documents in 38.19468329747518 minutes\n",
      "Tokenized 800000 documents in 38.66268664598465 minutes\n",
      "Tokenized 810000 documents in 39.153181703885394 minutes\n",
      "Tokenized 820000 documents in 39.63717047770818 minutes\n",
      "Tokenized 830000 documents in 40.12451574802399 minutes\n",
      "Tokenized 840000 documents in 40.599836723009744 minutes\n",
      "Tokenized 850000 documents in 41.08675314188004 minutes\n",
      "Tokenized 860000 documents in 41.5502089937528 minutes\n",
      "Tokenized 870000 documents in 42.02574290434519 minutes\n",
      "Tokenized 880000 documents in 42.5335600733757 minutes\n",
      "Tokenized 890000 documents in 43.01739630699158 minutes\n",
      "Tokenized 900000 documents in 43.510107866923015 minutes\n",
      "Tokenized 910000 documents in 43.9951574921608 minutes\n",
      "Tokenized 920000 documents in 44.472523800532024 minutes\n",
      "Tokenized 930000 documents in 44.96451102892558 minutes\n",
      "Tokenized 940000 documents in 45.45250288645426 minutes\n",
      "Tokenized 950000 documents in 45.93854943116506 minutes\n",
      "Tokenized 960000 documents in 46.42512188355128 minutes\n",
      "Tokenized 970000 documents in 46.90142422914505 minutes\n",
      "Tokenized 980000 documents in 47.38580068349838 minutes\n",
      "Tokenized 990000 documents in 47.87030677000681 minutes\n",
      "Tokenized 1000000 documents in 48.35338335037231 minutes\n",
      "Tokenized 1010000 documents in 48.823927998542786 minutes\n",
      "Tokenized 1020000 documents in 49.32722324927648 minutes\n",
      "Tokenized 1030000 documents in 49.82593594789505 minutes\n",
      "Tokenized 1040000 documents in 50.32156782150268 minutes\n",
      "Tokenized 1050000 documents in 50.822562567392985 minutes\n",
      "Tokenized 1060000 documents in 51.30907753308614 minutes\n",
      "Tokenized 1070000 documents in 51.787593110402426 minutes\n",
      "Tokenized 1080000 documents in 52.27736155589422 minutes\n",
      "Tokenized 1090000 documents in 52.75480731328329 minutes\n",
      "Tokenized 1100000 documents in 53.237277555465695 minutes\n",
      "Tokenized 1110000 documents in 53.70842061837514 minutes\n",
      "Tokenized 1120000 documents in 54.19998447497686 minutes\n",
      "Tokenized 1130000 documents in 54.67730504671733 minutes\n",
      "Tokenized 1140000 documents in 55.192935387293495 minutes\n",
      "Tokenized 1150000 documents in 55.65606701374054 minutes\n",
      "Tokenized 1160000 documents in 56.135895164807636 minutes\n",
      "Tokenized 1170000 documents in 56.63596030076345 minutes\n",
      "Tokenized 1180000 documents in 57.120426360766096 minutes\n",
      "Tokenized 1190000 documents in 57.61153082450231 minutes\n",
      "Tokenized 1200000 documents in 58.10835479895274 minutes\n",
      "Tokenized 1210000 documents in 58.60057866175969 minutes\n",
      "Tokenized 1220000 documents in 59.084546426932015 minutes\n",
      "Tokenized 1230000 documents in 59.57370548248291 minutes\n",
      "Tokenized 1240000 documents in 60.0581800142924 minutes\n",
      "Tokenized 1250000 documents in 60.5527521332105 minutes\n",
      "Tokenized 1260000 documents in 61.04944438139598 minutes\n",
      "Tokenized 1270000 documents in 61.552878657976784 minutes\n",
      "Tokenized 1280000 documents in 62.03117957115173 minutes\n",
      "Tokenized 1290000 documents in 62.536465934912364 minutes\n",
      "Tokenized 1300000 documents in 63.05552309354146 minutes\n",
      "Tokenized 1310000 documents in 63.539281757672626 minutes\n",
      "Tokenized 1320000 documents in 64.02577782869339 minutes\n",
      "Tokenized 1330000 documents in 64.5402236143748 minutes\n",
      "Tokenized 1340000 documents in 65.03990618387859 minutes\n",
      "Tokenized 1350000 documents in 65.53781810601552 minutes\n",
      "Tokenized 1360000 documents in 66.01929946343104 minutes\n",
      "Tokenized 1370000 documents in 66.49926480849584 minutes\n",
      "Tokenized 1380000 documents in 67.02626620531082 minutes\n",
      "Tokenized 1390000 documents in 67.53660480181377 minutes\n",
      "Tokenized 1400000 documents in 68.02246135075887 minutes\n",
      "Tokenized 1410000 documents in 68.50603532791138 minutes\n",
      "Tokenized 1420000 documents in 68.99960740407307 minutes\n",
      "Tokenized 1430000 documents in 69.47486878236136 minutes\n",
      "Tokenized 1440000 documents in 69.95321613152822 minutes\n",
      "Tokenized 1450000 documents in 70.4305642326673 minutes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized 1460000 documents in 70.93188039461772 minutes\n",
      "Tokenized 1470000 documents in 71.43803168535233 minutes\n",
      "Tokenized 1480000 documents in 71.91862063010534 minutes\n",
      "Tokenized 1490000 documents in 72.42247564792633 minutes\n",
      "Tokenized 1500000 documents in 72.90267002185186 minutes\n",
      "Tokenized 1510000 documents in 73.37509539524714 minutes\n",
      "Tokenized 1520000 documents in 73.86426709493001 minutes\n",
      "Tokenized 1530000 documents in 74.34241573015849 minutes\n",
      "Tokenized 1540000 documents in 74.85306224822997 minutes\n",
      "Tokenized 1550000 documents in 75.33233666817347 minutes\n",
      "Tokenized 1560000 documents in 75.80130147536596 minutes\n",
      "Tokenized 1570000 documents in 76.2842357993126 minutes\n",
      "Tokenized 1580000 documents in 76.75883768002193 minutes\n",
      "Tokenized 1590000 documents in 77.24144959847132 minutes\n",
      "Tokenized 1600000 documents in 77.73604729175568 minutes\n",
      "Tokenized 1610000 documents in 78.20423442522684 minutes\n",
      "Tokenized 1620000 documents in 78.64850689172745 minutes\n",
      "Tokenized 1630000 documents in 79.17324302196502 minutes\n",
      "Tokenized 1640000 documents in 79.63522875706354 minutes\n",
      "Tokenized 1650000 documents in 80.09140128691992 minutes\n",
      "Tokenized 1660000 documents in 80.56466101408004 minutes\n",
      "Tokenized 1670000 documents in 81.03243577480316 minutes\n",
      "Tokenized 1680000 documents in 81.52199623584747 minutes\n",
      "Tokenized 1690000 documents in 82.05180666446685 minutes\n",
      "Tokenized 1700000 documents in 82.52063866456349 minutes\n",
      "Tokenized 1710000 documents in 82.99122825463613 minutes\n",
      "Tokenized 1720000 documents in 83.45096635023752 minutes\n",
      "Tokenized 1730000 documents in 83.93448537190756 minutes\n",
      "Tokenized 1740000 documents in 84.40807129542033 minutes\n",
      "Tokenized 1750000 documents in 84.87852218548457 minutes\n",
      "Tokenized 1760000 documents in 85.34927937189738 minutes\n",
      "Tokenized 1770000 documents in 85.82008634010951 minutes\n",
      "Tokenized 1780000 documents in 86.2935299317042 minutes\n",
      "Tokenized 1790000 documents in 86.75827658176422 minutes\n",
      "Tokenized 1800000 documents in 87.25379383961359 minutes\n",
      "Tokenized 1810000 documents in 87.71085013151169 minutes\n",
      "Tokenized 1820000 documents in 88.16589194536209 minutes\n",
      "Tokenized 1830000 documents in 88.64148920377096 minutes\n",
      "Tokenized 1840000 documents in 89.10720367829005 minutes\n",
      "Tokenized 1850000 documents in 89.57655081351598 minutes\n",
      "Tokenized 1860000 documents in 90.05179802576701 minutes\n",
      "Tokenized 1870000 documents in 90.5483452240626 minutes\n",
      "Tokenized 1880000 documents in 91.007534857591 minutes\n",
      "Tokenized 1890000 documents in 91.46602409680685 minutes\n",
      "Tokenized 1900000 documents in 91.92709225416183 minutes\n",
      "Tokenized 1910000 documents in 92.38325003385543 minutes\n",
      "Tokenized 1920000 documents in 92.83896991411845 minutes\n",
      "Tokenized 1930000 documents in 93.29932667811711 minutes\n",
      "Tokenized 1940000 documents in 93.76773698329926 minutes\n",
      "Tokenized 1950000 documents in 94.24185567299524 minutes\n",
      "Tokenized 1960000 documents in 94.70292722384134 minutes\n",
      "Tokenized 1970000 documents in 95.17226376930873 minutes\n",
      "Tokenized 1980000 documents in 95.6335292816162 minutes\n",
      "Tokenized 1990000 documents in 96.09683797359466 minutes\n",
      "Tokenized 2000000 documents in 96.5703756848971 minutes\n",
      "Tokenized 2010000 documents in 97.04450658957164 minutes\n",
      "Tokenized 2020000 documents in 97.51927969058355 minutes\n",
      "Tokenized 2030000 documents in 97.98862979412078 minutes\n",
      "Tokenized 2040000 documents in 98.45822236537933 minutes\n",
      "Tokenized 2050000 documents in 98.92935516436894 minutes\n",
      "Tokenized 2060000 documents in 99.41217735608419 minutes\n",
      "Tokenized 2070000 documents in 99.89500093062719 minutes\n",
      "Tokenized 2080000 documents in 100.38445320924123 minutes\n",
      "Tokenized 2090000 documents in 100.87483812173208 minutes\n",
      "Tokenized 2100000 documents in 101.35208319822947 minutes\n",
      "Tokenized 2110000 documents in 101.84523666699728 minutes\n",
      "Tokenized 2120000 documents in 102.31237206459045 minutes\n",
      "Tokenized 2130000 documents in 102.78260256052017 minutes\n",
      "Tokenized 2140000 documents in 103.2578822851181 minutes\n",
      "Tokenized 2150000 documents in 103.73905980587006 minutes\n",
      "Tokenized 2160000 documents in 104.21567848126094 minutes\n",
      "Tokenized 2170000 documents in 104.700566025575 minutes\n",
      "Tokenized 2180000 documents in 105.1740906159083 minutes\n",
      "Tokenized 2190000 documents in 105.6805413444837 minutes\n",
      "Tokenized 2200000 documents in 106.16924479007722 minutes\n",
      "Tokenized 2210000 documents in 106.65593491395315 minutes\n",
      "Tokenized 2220000 documents in 107.14204958677291 minutes\n",
      "Tokenized 2230000 documents in 107.61779529651007 minutes\n",
      "Tokenized 2240000 documents in 108.07887357870737 minutes\n",
      "Tokenized 2250000 documents in 108.56192743380865 minutes\n",
      "Tokenized 2260000 documents in 109.03463163773219 minutes\n",
      "Tokenized 2270000 documents in 109.51584958632787 minutes\n",
      "Tokenized 2280000 documents in 110.00533434152604 minutes\n",
      "Tokenized 2290000 documents in 110.50306464831034 minutes\n",
      "Tokenized 2300000 documents in 110.98605664571126 minutes\n",
      "Tokenized 2310000 documents in 111.47538563013077 minutes\n",
      "Tokenized 2320000 documents in 111.96730771462123 minutes\n",
      "Tokenized 2330000 documents in 112.43178102572759 minutes\n",
      "Tokenized 2340000 documents in 112.93019338448842 minutes\n",
      "Tokenized 2350000 documents in 113.40725269317628 minutes\n",
      "Tokenized 2360000 documents in 113.8852083603541 minutes\n",
      "Tokenized 2370000 documents in 114.36829514106115 minutes\n",
      "Tokenized 2380000 documents in 114.9150903304418 minutes\n",
      "Tokenized 2390000 documents in 115.38209371566772 minutes\n",
      "Tokenized 2400000 documents in 115.86727546453476 minutes\n",
      "Tokenized 2410000 documents in 116.33001766602199 minutes\n",
      "Tokenized 2420000 documents in 116.8077599366506 minutes\n",
      "Tokenized 2430000 documents in 117.27287721236547 minutes\n",
      "Tokenized 2440000 documents in 117.75435867706935 minutes\n",
      "Tokenized 2450000 documents in 118.24809551239014 minutes\n",
      "Tokenized 2460000 documents in 118.7124604066213 minutes\n",
      "Tokenized 2470000 documents in 119.19254587491353 minutes\n",
      "Tokenized 2480000 documents in 119.68588832616805 minutes\n",
      "Tokenized 2490000 documents in 120.1758696715037 minutes\n",
      "Tokenized 2500000 documents in 120.6493374745051 minutes\n",
      "Tokenized 2510000 documents in 121.1176199833552 minutes\n",
      "Tokenized 2520000 documents in 121.58662322759628 minutes\n",
      "Tokenized 2530000 documents in 122.05268542369207 minutes\n",
      "Tokenized 2540000 documents in 122.52526163260141 minutes\n",
      "Tokenized 2550000 documents in 123.00749452114106 minutes\n",
      "Tokenized 2560000 documents in 123.51148470640183 minutes\n",
      "Tokenized 2570000 documents in 123.9957106312116 minutes\n",
      "Tokenized 2580000 documents in 124.4867525657018 minutes\n",
      "Tokenized 2590000 documents in 124.96810432275136 minutes\n",
      "Tokenized 2600000 documents in 125.4436810096105 minutes\n",
      "Tokenized 2610000 documents in 125.9204459865888 minutes\n",
      "Tokenized 2620000 documents in 126.39147559007009 minutes\n",
      "Tokenized 2630000 documents in 126.86525093714395 minutes\n",
      "Tokenized 2640000 documents in 127.34282705783843 minutes\n",
      "Tokenized 2650000 documents in 127.8128476858139 minutes\n",
      "Tokenized 2660000 documents in 128.29287064870198 minutes\n",
      "Tokenized 2670000 documents in 128.7430911620458 minutes\n",
      "Tokenized 2680000 documents in 129.21621077855428 minutes\n",
      "Tokenized 2690000 documents in 129.70777782996495 minutes\n",
      "Tokenized 2700000 documents in 130.1909990429878 minutes\n",
      "Tokenized 2710000 documents in 130.6711988369624 minutes\n",
      "Tokenized 2720000 documents in 131.16567976872128 minutes\n",
      "Tokenized 2730000 documents in 131.6408883412679 minutes\n",
      "Tokenized 2740000 documents in 132.11741841634114 minutes\n",
      "Tokenized 2750000 documents in 132.61169039408367 minutes\n",
      "Tokenized 2760000 documents in 133.0869686961174 minutes\n",
      "Tokenized 2770000 documents in 133.58054500023525 minutes\n",
      "Tokenized 2780000 documents in 134.07704639434814 minutes\n",
      "Tokenized 2790000 documents in 134.5550230383873 minutes\n",
      "Tokenized 2800000 documents in 135.04645395676295 minutes\n",
      "Tokenized 2810000 documents in 135.57429695129395 minutes\n",
      "Tokenized 2820000 documents in 136.05443089405696 minutes\n",
      "Tokenized 2830000 documents in 136.54910983641943 minutes\n",
      "Tokenized 2840000 documents in 137.04781534671784 minutes\n",
      "Tokenized 2850000 documents in 137.5618379632632 minutes\n",
      "Tokenized 2860000 documents in 138.03970926602682 minutes\n",
      "Tokenized 2870000 documents in 138.49180802504222 minutes\n",
      "Tokenized 2880000 documents in 139.01903276840847 minutes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized 2890000 documents in 139.53636140426 minutes\n",
      "Tokenized 2900000 documents in 140.015194050471 minutes\n",
      "Tokenized 2910000 documents in 140.49456522067388 minutes\n",
      "Tokenized 2920000 documents in 140.97068164348602 minutes\n",
      "Tokenized 2930000 documents in 141.4460195660591 minutes\n",
      "Tokenized 2940000 documents in 141.9182750582695 minutes\n",
      "Tokenized 2950000 documents in 142.4004859606425 minutes\n",
      "Tokenized 2960000 documents in 142.87431532144547 minutes\n",
      "Tokenized 2970000 documents in 143.35957545836766 minutes\n",
      "Tokenized 2980000 documents in 143.8504452228546 minutes\n",
      "Tokenized 2990000 documents in 144.32347902059556 minutes\n",
      "Tokenized 3000000 documents in 144.79202582041424 minutes\n",
      "Tokenized 3010000 documents in 145.2582317153613 minutes\n",
      "Tokenized 3020000 documents in 145.79376603364943 minutes\n",
      "Tokenized 3030000 documents in 146.2923243880272 minutes\n",
      "Tokenized 3040000 documents in 146.766107237339 minutes\n",
      "Tokenized 3050000 documents in 147.23065647681554 minutes\n",
      "Tokenized 3060000 documents in 147.70318824847539 minutes\n",
      "Tokenized 3070000 documents in 148.17482738097507 minutes\n",
      "Tokenized 3080000 documents in 148.65588517586392 minutes\n",
      "Tokenized 3090000 documents in 149.13287866512934 minutes\n",
      "Tokenized 3100000 documents in 149.56381260156633 minutes\n"
     ]
    }
   ],
   "source": [
    "filtered_tokens = []\n",
    "start = time.time()\n",
    "i = 1\n",
    "for doc in nlp.pipe(docs, disable=['tagger', 'parser', 'ner', 'textcat'], batch_size=10000):\n",
    "    try:\n",
    "        tokens = [token.lemma_ for token in doc if token_filter(token)]\n",
    "        filtered_tokens.append(tokens)\n",
    "        i += 1\n",
    "        if i % 10000 == 0:\n",
    "            print(f'Tokenized {i} documents in {(time.time()-start)/60} minutes')\n",
    "    except:\n",
    "        print(f'Document {i} has an encoding error/has error characters.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the tokenized restaurant reviews to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/tokenized.pkl', 'wb') as f:\n",
    "    pickle.dump(filtered_tokens, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = businesses['processed'].astype('str').astype('unicode').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized 10000 documents in 0.5435734550158183 minutes\n",
      "Tokenized 20000 documents in 1.0860312422116598 minutes\n",
      "Tokenized 30000 documents in 1.6481721639633178 minutes\n",
      "Tokenized 40000 documents in 2.205586814880371 minutes\n",
      "Tokenized 50000 documents in 2.7536702275276186 minutes\n",
      "Tokenized 60000 documents in 3.290149998664856 minutes\n",
      "Tokenized 70000 documents in 3.8397332151730854 minutes\n",
      "Tokenized 80000 documents in 4.389596164226532 minutes\n",
      "Tokenized 90000 documents in 4.932957581679026 minutes\n",
      "Tokenized 100000 documents in 5.466143671671549 minutes\n",
      "Tokenized 110000 documents in 6.000996728738149 minutes\n",
      "Tokenized 120000 documents in 6.546352938810984 minutes\n",
      "Tokenized 130000 documents in 7.077045182387034 minutes\n",
      "Tokenized 140000 documents in 7.6094328244527185 minutes\n",
      "Tokenized 150000 documents in 8.157556692759195 minutes\n",
      "Tokenized 160000 documents in 8.699893486499786 minutes\n",
      "Tokenized 170000 documents in 9.256111359596252 minutes\n",
      "Tokenized 180000 documents in 9.816303678353627 minutes\n",
      "Tokenized 190000 documents in 10.373857116699218 minutes\n",
      "Tokenized 200000 documents in 10.918476982911427 minutes\n",
      "Tokenized 210000 documents in 11.467291593551636 minutes\n",
      "Tokenized 220000 documents in 12.00387909412384 minutes\n",
      "Tokenized 230000 documents in 12.550351492563884 minutes\n",
      "Tokenized 240000 documents in 13.099249796072643 minutes\n",
      "Tokenized 250000 documents in 13.657659355799357 minutes\n",
      "Tokenized 260000 documents in 14.212891586621602 minutes\n",
      "Tokenized 270000 documents in 14.752832925319671 minutes\n",
      "Tokenized 280000 documents in 15.288612377643584 minutes\n",
      "Tokenized 290000 documents in 15.842842427889506 minutes\n",
      "Tokenized 300000 documents in 16.39165170987447 minutes\n",
      "Tokenized 310000 documents in 16.94897368748983 minutes\n",
      "Tokenized 320000 documents in 17.49651142756144 minutes\n",
      "Tokenized 330000 documents in 18.03988039890925 minutes\n",
      "Tokenized 340000 documents in 18.57594463030497 minutes\n",
      "Tokenized 350000 documents in 19.132132097085318 minutes\n",
      "Tokenized 360000 documents in 19.689269506931304 minutes\n",
      "Tokenized 370000 documents in 20.230436062812807 minutes\n",
      "Tokenized 380000 documents in 20.76684565146764 minutes\n",
      "Tokenized 390000 documents in 21.31046366294225 minutes\n",
      "Tokenized 400000 documents in 21.85347071091334 minutes\n",
      "Tokenized 410000 documents in 22.394336024920147 minutes\n",
      "Tokenized 420000 documents in 22.94410920937856 minutes\n",
      "Tokenized 430000 documents in 23.48982007106145 minutes\n",
      "Tokenized 440000 documents in 24.035731343428292 minutes\n",
      "Tokenized 450000 documents in 24.592196023464204 minutes\n",
      "Tokenized 460000 documents in 25.1343218366305 minutes\n",
      "Tokenized 470000 documents in 25.67452702522278 minutes\n",
      "Tokenized 480000 documents in 26.214923902352652 minutes\n",
      "Tokenized 490000 documents in 26.753453465302787 minutes\n",
      "Tokenized 500000 documents in 27.294484261671702 minutes\n",
      "Tokenized 510000 documents in 27.85767789284388 minutes\n",
      "Tokenized 520000 documents in 28.40348494052887 minutes\n",
      "Tokenized 530000 documents in 28.943283247947694 minutes\n",
      "Tokenized 540000 documents in 29.480838712056478 minutes\n",
      "Tokenized 550000 documents in 30.033428303400676 minutes\n",
      "Tokenized 560000 documents in 30.55214672088623 minutes\n",
      "Tokenized 570000 documents in 31.079036247730254 minutes\n",
      "Tokenized 580000 documents in 31.60450135866801 minutes\n",
      "Tokenized 590000 documents in 32.13247971932093 minutes\n",
      "Tokenized 600000 documents in 32.65314798752467 minutes\n",
      "Tokenized 610000 documents in 33.172116963068646 minutes\n",
      "Tokenized 620000 documents in 33.68592527310054 minutes\n",
      "Tokenized 630000 documents in 34.208634400367735 minutes\n",
      "Tokenized 640000 documents in 34.72026325066884 minutes\n",
      "Tokenized 650000 documents in 35.22463024059932 minutes\n",
      "Tokenized 660000 documents in 35.743292506535845 minutes\n",
      "Tokenized 670000 documents in 36.25730067888896 minutes\n",
      "Tokenized 680000 documents in 36.768117705980934 minutes\n",
      "Tokenized 690000 documents in 37.283860508600874 minutes\n",
      "Tokenized 700000 documents in 37.80680345694224 minutes\n",
      "Tokenized 710000 documents in 38.33265951871872 minutes\n",
      "Tokenized 720000 documents in 38.87740842501322 minutes\n",
      "Tokenized 730000 documents in 39.39959725936254 minutes\n",
      "Tokenized 740000 documents in 39.91456342140834 minutes\n",
      "Tokenized 750000 documents in 40.41594475905101 minutes\n",
      "Tokenized 760000 documents in 40.93209542830785 minutes\n",
      "Tokenized 770000 documents in 41.474589403470354 minutes\n",
      "Tokenized 780000 documents in 41.995663873354594 minutes\n",
      "Tokenized 790000 documents in 42.50338796774546 minutes\n",
      "Tokenized 800000 documents in 43.016239420572916 minutes\n",
      "Tokenized 810000 documents in 43.5335484902064 minutes\n",
      "Tokenized 820000 documents in 44.050868793328604 minutes\n",
      "Tokenized 830000 documents in 44.58311580816905 minutes\n",
      "Tokenized 840000 documents in 45.094548034667966 minutes\n",
      "Tokenized 850000 documents in 45.60331695874532 minutes\n",
      "Tokenized 860000 documents in 46.1215668241183 minutes\n",
      "Tokenized 870000 documents in 46.6377985517184 minutes\n",
      "Tokenized 880000 documents in 47.14311620394389 minutes\n",
      "Tokenized 890000 documents in 47.634498631954195 minutes\n",
      "Tokenized 900000 documents in 48.14804265499115 minutes\n",
      "Tokenized 910000 documents in 48.68112353086472 minutes\n",
      "Tokenized 920000 documents in 49.201269586881004 minutes\n",
      "Tokenized 930000 documents in 49.71877468029658 minutes\n",
      "Tokenized 940000 documents in 50.23324116865794 minutes\n",
      "Tokenized 950000 documents in 50.772737137476604 minutes\n",
      "Tokenized 960000 documents in 51.28687292734782 minutes\n",
      "Tokenized 970000 documents in 51.821917017300926 minutes\n",
      "Tokenized 980000 documents in 52.339746344089505 minutes\n",
      "Tokenized 990000 documents in 52.83998555342357 minutes\n",
      "Tokenized 1000000 documents in 53.36998917261759 minutes\n",
      "Tokenized 1010000 documents in 53.87856907447179 minutes\n",
      "Tokenized 1020000 documents in 54.37487380504608 minutes\n",
      "Tokenized 1030000 documents in 54.87929566303889 minutes\n",
      "Tokenized 1040000 documents in 55.3855681180954 minutes\n",
      "Tokenized 1050000 documents in 55.89172460238139 minutes\n",
      "Tokenized 1060000 documents in 56.40761994918187 minutes\n",
      "Tokenized 1070000 documents in 56.91125312248866 minutes\n",
      "Tokenized 1080000 documents in 57.42871243953705 minutes\n",
      "Tokenized 1090000 documents in 57.943508525689445 minutes\n",
      "Tokenized 1100000 documents in 58.46503570874532 minutes\n",
      "Tokenized 1110000 documents in 58.974356595675154 minutes\n",
      "Tokenized 1120000 documents in 59.49228002230326 minutes\n",
      "Tokenized 1130000 documents in 59.990596330165864 minutes\n",
      "Tokenized 1140000 documents in 60.48649391730626 minutes\n",
      "Tokenized 1150000 documents in 61.0187669634819 minutes\n",
      "Tokenized 1160000 documents in 61.552483506997426 minutes\n",
      "Tokenized 1170000 documents in 62.08710007667541 minutes\n",
      "Tokenized 1180000 documents in 62.618194564183554 minutes\n",
      "Tokenized 1190000 documents in 63.15841881831487 minutes\n",
      "Tokenized 1200000 documents in 63.685957495371504 minutes\n",
      "Tokenized 1210000 documents in 64.21238708496094 minutes\n",
      "Tokenized 1220000 documents in 64.73682899077734 minutes\n",
      "Tokenized 1230000 documents in 65.2824171702067 minutes\n",
      "Tokenized 1240000 documents in 65.79464982748031 minutes\n",
      "Tokenized 1250000 documents in 66.31065170764923 minutes\n",
      "Tokenized 1260000 documents in 66.83611984650294 minutes\n",
      "Tokenized 1270000 documents in 67.34459079901377 minutes\n",
      "Tokenized 1280000 documents in 67.84978712399801 minutes\n",
      "Tokenized 1290000 documents in 68.36888552506765 minutes\n",
      "Tokenized 1300000 documents in 68.87946963707606 minutes\n",
      "Tokenized 1310000 documents in 69.41652551492055 minutes\n",
      "Tokenized 1320000 documents in 69.99123393694559 minutes\n",
      "Tokenized 1330000 documents in 70.5591677069664 minutes\n",
      "Tokenized 1340000 documents in 71.12137570778529 minutes\n",
      "Tokenized 1350000 documents in 71.69664483467737 minutes\n",
      "Tokenized 1360000 documents in 72.2652119119962 minutes\n",
      "Tokenized 1370000 documents in 72.82068562904993 minutes\n",
      "Tokenized 1380000 documents in 73.376822245121 minutes\n",
      "Tokenized 1390000 documents in 73.93778088092805 minutes\n",
      "Tokenized 1400000 documents in 74.48807205359141 minutes\n",
      "Tokenized 1410000 documents in 75.0353040019671 minutes\n",
      "Tokenized 1420000 documents in 75.58290496667226 minutes\n",
      "Tokenized 1430000 documents in 76.13531284332275 minutes\n",
      "Tokenized 1440000 documents in 76.68503654400507 minutes\n",
      "Tokenized 1450000 documents in 77.23675913413366 minutes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized 1460000 documents in 77.79469596544901 minutes\n",
      "Tokenized 1470000 documents in 78.3345144867897 minutes\n",
      "Tokenized 1480000 documents in 78.86865790287654 minutes\n",
      "Tokenized 1490000 documents in 79.43154362440109 minutes\n",
      "Tokenized 1500000 documents in 79.97490022977193 minutes\n",
      "Tokenized 1510000 documents in 80.51221429109573 minutes\n",
      "Tokenized 1520000 documents in 81.03863857189815 minutes\n",
      "Tokenized 1530000 documents in 81.5610393246015 minutes\n",
      "Tokenized 1540000 documents in 82.09267423550288 minutes\n",
      "Tokenized 1550000 documents in 82.64216370979945 minutes\n",
      "Tokenized 1560000 documents in 83.17951014041901 minutes\n",
      "Tokenized 1570000 documents in 83.70984456539153 minutes\n",
      "Tokenized 1580000 documents in 84.27566196918488 minutes\n",
      "Tokenized 1590000 documents in 84.79718762636185 minutes\n",
      "Tokenized 1600000 documents in 85.31929559310278 minutes\n",
      "Tokenized 1610000 documents in 85.83360180854797 minutes\n",
      "Tokenized 1620000 documents in 86.34297099113465 minutes\n",
      "Tokenized 1630000 documents in 86.86022646824519 minutes\n",
      "Tokenized 1640000 documents in 87.37007601658503 minutes\n",
      "Tokenized 1650000 documents in 87.87350899378458 minutes\n",
      "Tokenized 1660000 documents in 88.37804731925328 minutes\n",
      "Tokenized 1670000 documents in 88.89294563134511 minutes\n",
      "Tokenized 1680000 documents in 89.4142414132754 minutes\n",
      "Tokenized 1690000 documents in 89.93598164319992 minutes\n",
      "Tokenized 1700000 documents in 90.48697965542475 minutes\n",
      "Tokenized 1710000 documents in 91.04019741217296 minutes\n",
      "Tokenized 1720000 documents in 91.57972952524821 minutes\n",
      "Tokenized 1730000 documents in 92.12186671892802 minutes\n",
      "Tokenized 1740000 documents in 92.6829052845637 minutes\n",
      "Tokenized 1750000 documents in 93.22871538003285 minutes\n",
      "Tokenized 1760000 documents in 93.75887933572133 minutes\n",
      "Tokenized 1770000 documents in 94.30074041684469 minutes\n",
      "Tokenized 1780000 documents in 94.8453018506368 minutes\n",
      "Tokenized 1790000 documents in 95.39231230020523 minutes\n",
      "Tokenized 1800000 documents in 95.936927652359 minutes\n",
      "Tokenized 1810000 documents in 96.47250046730042 minutes\n",
      "Tokenized 1820000 documents in 97.00367238124211 minutes\n",
      "Tokenized 1830000 documents in 97.53875095844269 minutes\n",
      "Tokenized 1840000 documents in 98.06591091950735 minutes\n",
      "Tokenized 1850000 documents in 98.58638070821762 minutes\n",
      "Tokenized 1860000 documents in 99.10535726944606 minutes\n",
      "Tokenized 1870000 documents in 99.63158736228942 minutes\n",
      "Tokenized 1880000 documents in 100.16854422887167 minutes\n",
      "Tokenized 1890000 documents in 100.68039538860322 minutes\n",
      "Tokenized 1900000 documents in 101.19567084312439 minutes\n",
      "Tokenized 1910000 documents in 101.71910492181777 minutes\n",
      "Tokenized 1920000 documents in 102.25010943015417 minutes\n",
      "Tokenized 1930000 documents in 102.76727349360785 minutes\n",
      "Tokenized 1940000 documents in 103.29488331476847 minutes\n",
      "Tokenized 1950000 documents in 103.79751561880111 minutes\n",
      "Tokenized 1960000 documents in 104.29266035556793 minutes\n",
      "Tokenized 1970000 documents in 104.8062488913536 minutes\n",
      "Tokenized 1980000 documents in 105.31548595825831 minutes\n",
      "Tokenized 1990000 documents in 105.82110296090444 minutes\n",
      "Tokenized 2000000 documents in 106.32510035037994 minutes\n"
     ]
    }
   ],
   "source": [
    "filtered_tokens_bs = []\n",
    "start = time.time()\n",
    "i = 1\n",
    "for doc in nlp.pipe(docs, disable=['tagger', 'parser', 'ner', 'textcat'], batch_size=10000):\n",
    "    try:\n",
    "        tokens = [token.lemma_ for token in doc if token_filter(token)]\n",
    "        filtered_tokens_bs.append(tokens)\n",
    "        i += 1\n",
    "        if i % 10000 == 0:\n",
    "            print(f'Tokenized {i} documents in {(time.time()-start)/60} minutes')\n",
    "    except:\n",
    "        print(f'Document {i} has an encoding error.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/tokenized_bs.pkl', 'wb') as f:\n",
    "    pickle.dump(filtered_tokens_bs, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
