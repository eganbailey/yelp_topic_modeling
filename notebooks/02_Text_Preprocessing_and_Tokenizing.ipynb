{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing and Tokenizing\n",
    "\n",
    "In this notebook, we process and tokenize each review text. In the preprocessing stage, the text is converted to lowercase and words are lemmatized. In the tokenizing stage, each review is converted into a document that contains single words as tokens. These documents are then compiled into a corpus.\n",
    "\n",
    "Since restaurants dominate the Yelp review space, two corpuses are constructed: one for restaurants, and one for all non-restaurant businesses.\n",
    "\n",
    "## Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "import textacy\n",
    "import pickle\n",
    "import time\n",
    "import string\n",
    "import unidecode\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%run .engine.py\n",
    "engine = create_engine(LOGIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "### Restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rests = pd.read_csv('../data/restaurants.csv', compression='gzip', usecols=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3055990, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rests.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query for Other Businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "businesses = pd.read_csv('../data/businesses.csv', compression='gzip', usecols=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1968973, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "businesses.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the text\n",
    "\n",
    "I use textacy's preprocess method to convert all the text to lowercase and remove numbers, URLs, and punctuation.\n",
    "\n",
    "### Restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23min 56s, sys: 1.62 s, total: 23min 57s\n",
      "Wall time: 23min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rests['processed'] = rests['text'].map(lambda x: textacy.preprocess.preprocess_text(x, lowercase=True, no_urls=True, \n",
    "                                                                                    no_punct=True, no_numbers=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15min 17s, sys: 1.24 s, total: 15min 18s\n",
      "Wall time: 15min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "businesses['processed'] = businesses['text'].map(lambda x: textacy.preprocess.preprocess_text(x, lowercase=True, no_urls=True,\n",
    "                                                                                              no_punct=True, no_numbers=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for non-English reviews\n",
    "\n",
    "Since Yelp is used worldwide, we should check to see if any reviews were written using non-English characters as non-English words may not be tokenized or processed correctly. I define the function `isEnglish` to filter out non-ASCII characters as non-ASCII characters would predominantly be used by users typing reviews in non-English languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define `isEnglish` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode('ascii')\n",
    "    except UnicodeEncodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rests['isEnglish'] = rests['processed'].astype('str').astype('unicode').apply(lambda x: isEnglish(x) == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     2984421\n",
       "False      71569\n",
       "Name: isEnglish, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rests['isEnglish'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rests_english = rests[rests['isEnglish'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../data/rests_eng_index.npy', rests[rests['isEnglish'] == True].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "businesses['isEnglish'] = businesses['processed'].astype('str').astype('unicode').apply(lambda x: isEnglish(x) == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     1952542\n",
       "False      16431\n",
       "Name: isEnglish, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "businesses['isEnglish'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../data/bus_eng_index.npy', businesses[businesses['isEnglish'] == True].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing\n",
    "\n",
    "In this section I set up a spacy tokenizer. We disable part-of-speech tagging, semantic parsing, and text categorization to reduce overall memory usage, and choose to retain the lemmas of each token. We also create a filter function to eliminate stopwords and short tokens (less than 4 characters). The tokenized documents are then added to a list which we can pass through a vectorizer (see Notebook 3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = textacy.load_spacy(\"en_core_web_sm\", disable = (\"tagger\", \"parser\", \"ner\", \"textcat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_filter(token): #remove stopwords and tokens 4 char or less\n",
    "    return not (token.is_stop | len(token.text) <= 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = rests['processed'].astype('str').astype('unicode').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized 10000 documents in 0.5251728336016337 minutes\n",
      "Tokenized 20000 documents in 1.0496850768725077 minutes\n",
      "Tokenized 30000 documents in 1.579992691675822 minutes\n",
      "Tokenized 40000 documents in 2.0894193092981976 minutes\n",
      "Tokenized 50000 documents in 2.5953927556673686 minutes\n",
      "Tokenized 60000 documents in 3.1252748092015583 minutes\n",
      "Tokenized 70000 documents in 3.643654414017995 minutes\n",
      "Tokenized 80000 documents in 4.167919055620829 minutes\n",
      "Tokenized 90000 documents in 4.67060960928599 minutes\n",
      "Tokenized 100000 documents in 5.168066438039144 minutes\n",
      "Tokenized 110000 documents in 5.657072214285533 minutes\n",
      "Tokenized 120000 documents in 6.167018107573191 minutes\n",
      "Tokenized 130000 documents in 6.684710649649302 minutes\n",
      "Tokenized 140000 documents in 7.20735391775767 minutes\n",
      "Tokenized 150000 documents in 7.725206498305003 minutes\n",
      "Tokenized 160000 documents in 8.251602478822072 minutes\n",
      "Tokenized 210000 documents in 10.8531729499499 minutes\n",
      "Tokenized 220000 documents in 11.353967575232188 minutes\n",
      "Tokenized 230000 documents in 11.888592290878297 minutes\n",
      "Tokenized 240000 documents in 12.410729817549388 minutes\n",
      "Tokenized 250000 documents in 12.942264835039774 minutes\n",
      "Tokenized 260000 documents in 13.450786900520324 minutes\n",
      "Tokenized 270000 documents in 13.9668536901474 minutes\n",
      "Tokenized 280000 documents in 14.471694012482962 minutes\n",
      "Tokenized 290000 documents in 14.979430309931438 minutes\n",
      "Tokenized 300000 documents in 15.49200142621994 minutes\n",
      "Tokenized 310000 documents in 16.002184466520944 minutes\n",
      "Tokenized 320000 documents in 16.511803619066875 minutes\n",
      "Tokenized 330000 documents in 17.022384385267895 minutes\n",
      "Tokenized 340000 documents in 17.54425619840622 minutes\n",
      "Tokenized 350000 documents in 18.050692180792492 minutes\n",
      "Tokenized 360000 documents in 18.563619673252106 minutes\n",
      "Tokenized 370000 documents in 19.0549178759257 minutes\n",
      "Tokenized 380000 documents in 19.560432545344035 minutes\n",
      "Tokenized 390000 documents in 20.073445228735604 minutes\n",
      "Tokenized 400000 documents in 20.566891566912332 minutes\n",
      "Tokenized 410000 documents in 21.079988698164623 minutes\n",
      "Tokenized 420000 documents in 21.59088643391927 minutes\n",
      "Tokenized 430000 documents in 22.097338036696115 minutes\n",
      "Tokenized 440000 documents in 22.593368673324584 minutes\n",
      "Tokenized 450000 documents in 23.11107136408488 minutes\n",
      "Tokenized 460000 documents in 23.604117639859517 minutes\n",
      "Tokenized 470000 documents in 24.098590274651844 minutes\n",
      "Tokenized 480000 documents in 24.606950569152833 minutes\n",
      "Tokenized 490000 documents in 25.124396820863087 minutes\n",
      "Tokenized 500000 documents in 25.626827987035117 minutes\n",
      "Tokenized 510000 documents in 26.116568076610566 minutes\n",
      "Tokenized 520000 documents in 26.612147295475005 minutes\n",
      "Tokenized 530000 documents in 27.0897403438886 minutes\n",
      "Tokenized 540000 documents in 27.581725172201793 minutes\n",
      "Tokenized 550000 documents in 28.052447545528413 minutes\n",
      "Tokenized 560000 documents in 28.526336081822713 minutes\n",
      "Tokenized 570000 documents in 28.995541409651437 minutes\n",
      "Tokenized 580000 documents in 29.466439187526703 minutes\n",
      "Tokenized 590000 documents in 29.926762656370798 minutes\n",
      "Tokenized 600000 documents in 30.385940047105155 minutes\n",
      "Tokenized 610000 documents in 30.858956535657246 minutes\n",
      "Tokenized 620000 documents in 31.330724759896597 minutes\n",
      "Tokenized 630000 documents in 31.81137115160624 minutes\n",
      "Tokenized 640000 documents in 32.29585843880971 minutes\n",
      "Tokenized 650000 documents in 32.768107664585116 minutes\n",
      "Tokenized 660000 documents in 33.283741641044614 minutes\n",
      "Tokenized 670000 documents in 33.75677528778712 minutes\n",
      "Tokenized 680000 documents in 34.24250322977702 minutes\n",
      "Tokenized 690000 documents in 34.7286059141159 minutes\n",
      "Tokenized 700000 documents in 35.21031753619512 minutes\n",
      "Tokenized 710000 documents in 35.67885419925054 minutes\n",
      "Tokenized 720000 documents in 36.1541419227918 minutes\n",
      "Tokenized 730000 documents in 36.63332882324855 minutes\n",
      "Tokenized 740000 documents in 37.116468318303426 minutes\n",
      "Tokenized 750000 documents in 37.609219602743785 minutes\n",
      "Tokenized 760000 documents in 38.087791657447816 minutes\n",
      "Tokenized 770000 documents in 38.563072605927786 minutes\n",
      "Tokenized 780000 documents in 39.04964943726858 minutes\n",
      "Tokenized 790000 documents in 39.53281634251277 minutes\n",
      "Tokenized 800000 documents in 40.0428174217542 minutes\n",
      "Tokenized 810000 documents in 40.53707389831543 minutes\n",
      "Tokenized 820000 documents in 41.026090784867606 minutes\n",
      "Tokenized 830000 documents in 41.506171596050265 minutes\n",
      "Tokenized 840000 documents in 41.9889132420222 minutes\n",
      "Tokenized 850000 documents in 42.463796162605284 minutes\n",
      "Tokenized 860000 documents in 42.93476286331813 minutes\n",
      "Tokenized 870000 documents in 43.44637697140376 minutes\n",
      "Tokenized 880000 documents in 43.90990554889043 minutes\n",
      "Tokenized 890000 documents in 44.41369025309881 minutes\n",
      "Tokenized 900000 documents in 44.88088580767314 minutes\n",
      "Tokenized 910000 documents in 45.35356707175573 minutes\n",
      "Tokenized 920000 documents in 45.81881733338038 minutes\n",
      "Tokenized 930000 documents in 46.2910694360733 minutes\n",
      "Tokenized 940000 documents in 46.737077391147615 minutes\n",
      "Tokenized 950000 documents in 47.204130391279854 minutes\n",
      "Tokenized 960000 documents in 47.666871531804404 minutes\n",
      "Tokenized 970000 documents in 48.13471357822418 minutes\n",
      "Tokenized 980000 documents in 48.61775238513947 minutes\n",
      "Tokenized 990000 documents in 49.09803246657054 minutes\n",
      "Tokenized 1000000 documents in 49.58649440209071 minutes\n",
      "Tokenized 1010000 documents in 50.07929468552272 minutes\n",
      "Tokenized 1020000 documents in 50.55865600506465 minutes\n",
      "Tokenized 1030000 documents in 51.04425831238429 minutes\n",
      "Tokenized 1040000 documents in 51.51997937361399 minutes\n",
      "Tokenized 1050000 documents in 51.990045205752054 minutes\n",
      "Tokenized 1060000 documents in 52.45978546142578 minutes\n",
      "Tokenized 1070000 documents in 52.933094716072084 minutes\n",
      "Tokenized 1080000 documents in 53.40489246050517 minutes\n",
      "Tokenized 1090000 documents in 53.88970632950465 minutes\n",
      "Tokenized 1100000 documents in 54.37010580301285 minutes\n",
      "Tokenized 1110000 documents in 54.84673931598663 minutes\n",
      "Tokenized 1120000 documents in 55.3248836795489 minutes\n",
      "Tokenized 1130000 documents in 55.85204785664876 minutes\n",
      "Tokenized 1140000 documents in 56.3384268840154 minutes\n",
      "Tokenized 1150000 documents in 56.841784818967184 minutes\n",
      "Tokenized 1160000 documents in 57.32677590052287 minutes\n",
      "Tokenized 1170000 documents in 57.80959455966949 minutes\n",
      "Tokenized 1180000 documents in 58.288300132751466 minutes\n",
      "Tokenized 1190000 documents in 58.763702142238614 minutes\n",
      "Tokenized 1200000 documents in 59.25236657857895 minutes\n",
      "Tokenized 1210000 documents in 59.7279003183047 minutes\n",
      "Tokenized 1220000 documents in 60.19549988508224 minutes\n",
      "Tokenized 1230000 documents in 60.68283332188924 minutes\n",
      "Tokenized 1240000 documents in 61.16632712682088 minutes\n",
      "Tokenized 1250000 documents in 61.63039564291636 minutes\n",
      "Tokenized 1260000 documents in 62.102318112055464 minutes\n",
      "Tokenized 1270000 documents in 62.56802118619283 minutes\n",
      "Tokenized 1280000 documents in 63.02784786621729 minutes\n",
      "Tokenized 1290000 documents in 63.488546820481616 minutes\n",
      "Tokenized 1300000 documents in 63.95975427627563 minutes\n",
      "Tokenized 1310000 documents in 64.42646329800287 minutes\n",
      "Tokenized 1320000 documents in 64.90381826559702 minutes\n",
      "Tokenized 1330000 documents in 65.39360992908477 minutes\n",
      "Tokenized 1340000 documents in 65.87996549606324 minutes\n",
      "Tokenized 1350000 documents in 66.36273908217748 minutes\n",
      "Tokenized 1360000 documents in 66.86940043767294 minutes\n",
      "Tokenized 1370000 documents in 67.34671645959219 minutes\n",
      "Tokenized 1380000 documents in 67.84351335763931 minutes\n",
      "Tokenized 1390000 documents in 68.3270038485527 minutes\n",
      "Tokenized 1400000 documents in 68.81463186343511 minutes\n",
      "Tokenized 1410000 documents in 69.29315569003423 minutes\n",
      "Tokenized 1420000 documents in 69.77255835533143 minutes\n",
      "Tokenized 1430000 documents in 70.26209526459375 minutes\n",
      "Tokenized 1440000 documents in 70.73542287349701 minutes\n",
      "Tokenized 1450000 documents in 71.22823218504588 minutes\n",
      "Tokenized 1460000 documents in 71.75767156283061 minutes\n",
      "Tokenized 1470000 documents in 72.24444266160329 minutes\n",
      "Tokenized 1480000 documents in 72.72252558072408 minutes\n",
      "Tokenized 1490000 documents in 73.21209559043248 minutes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized 1500000 documents in 73.7062535405159 minutes\n",
      "Tokenized 1510000 documents in 74.20094054937363 minutes\n",
      "Tokenized 1520000 documents in 74.70778512557348 minutes\n",
      "Tokenized 1530000 documents in 75.20267874002457 minutes\n",
      "Tokenized 1540000 documents in 75.68102876345317 minutes\n",
      "Tokenized 1550000 documents in 76.16191310882569 minutes\n",
      "Tokenized 1560000 documents in 76.6355654199918 minutes\n",
      "Tokenized 1570000 documents in 77.11772193908692 minutes\n",
      "Tokenized 1580000 documents in 77.5879980802536 minutes\n",
      "Tokenized 1590000 documents in 78.06524838209153 minutes\n",
      "Tokenized 1600000 documents in 78.55346725781759 minutes\n",
      "Tokenized 1610000 documents in 79.03019727071127 minutes\n",
      "Tokenized 1620000 documents in 79.50208503007889 minutes\n",
      "Tokenized 1630000 documents in 79.97397522131602 minutes\n",
      "Tokenized 1640000 documents in 80.44068801403046 minutes\n",
      "Tokenized 1650000 documents in 80.8973538160324 minutes\n",
      "Tokenized 1660000 documents in 81.35792133808135 minutes\n",
      "Tokenized 1670000 documents in 81.8187983751297 minutes\n",
      "Tokenized 1680000 documents in 82.3055310845375 minutes\n",
      "Tokenized 1690000 documents in 82.79628034432729 minutes\n",
      "Tokenized 1700000 documents in 83.2730565627416 minutes\n",
      "Tokenized 1710000 documents in 83.74942380984625 minutes\n",
      "Tokenized 1720000 documents in 84.23924227158228 minutes\n",
      "Tokenized 1730000 documents in 84.71910943984986 minutes\n",
      "Tokenized 1740000 documents in 85.20471192598343 minutes\n",
      "Tokenized 1750000 documents in 85.69149573246638 minutes\n",
      "Tokenized 1760000 documents in 86.17212223609289 minutes\n",
      "Tokenized 1770000 documents in 86.64555565516154 minutes\n",
      "Tokenized 1780000 documents in 87.12993474404017 minutes\n",
      "Tokenized 1790000 documents in 87.61452910502751 minutes\n",
      "Tokenized 1800000 documents in 88.09902111291885 minutes\n",
      "Tokenized 1810000 documents in 88.58844689528148 minutes\n",
      "Tokenized 1820000 documents in 89.06889989773433 minutes\n",
      "Tokenized 1830000 documents in 89.54946777423223 minutes\n",
      "Tokenized 1840000 documents in 90.03629418611527 minutes\n",
      "Tokenized 1850000 documents in 90.53211359977722 minutes\n",
      "Tokenized 1860000 documents in 91.02511305411657 minutes\n",
      "Tokenized 1870000 documents in 91.5923489411672 minutes\n",
      "Tokenized 1880000 documents in 92.0887748559316 minutes\n",
      "Tokenized 1890000 documents in 92.57121068636576 minutes\n",
      "Tokenized 1900000 documents in 93.05506389538446 minutes\n",
      "Tokenized 1910000 documents in 93.53780049482981 minutes\n",
      "Tokenized 1920000 documents in 94.0109346071879 minutes\n",
      "Tokenized 1930000 documents in 94.48915090958278 minutes\n",
      "Tokenized 1940000 documents in 94.96101660728455 minutes\n",
      "Tokenized 1950000 documents in 95.44399735927581 minutes\n",
      "Tokenized 1960000 documents in 95.91803761720658 minutes\n",
      "Tokenized 1970000 documents in 96.40069764852524 minutes\n",
      "Tokenized 1980000 documents in 96.87248480717341 minutes\n",
      "Tokenized 1990000 documents in 97.34483243624369 minutes\n",
      "Tokenized 2000000 documents in 97.80905420382818 minutes\n",
      "Tokenized 2010000 documents in 98.27788616418839 minutes\n",
      "Tokenized 2020000 documents in 98.75501940250396 minutes\n",
      "Tokenized 2030000 documents in 99.23237111171086 minutes\n",
      "Tokenized 2040000 documents in 99.71556188662846 minutes\n",
      "Tokenized 2050000 documents in 100.19213542143504 minutes\n",
      "Tokenized 2060000 documents in 100.67453450759253 minutes\n",
      "Tokenized 2070000 documents in 101.16241925557455 minutes\n",
      "Tokenized 2080000 documents in 101.63954668839773 minutes\n",
      "Tokenized 2090000 documents in 102.11854368448257 minutes\n",
      "Tokenized 2100000 documents in 102.6049167116483 minutes\n",
      "Tokenized 2110000 documents in 103.09392335414887 minutes\n",
      "Tokenized 2120000 documents in 103.56605085531871 minutes\n",
      "Tokenized 2130000 documents in 104.0485115369161 minutes\n",
      "Tokenized 2140000 documents in 104.52830443382263 minutes\n",
      "Tokenized 2150000 documents in 105.00463056166967 minutes\n",
      "Tokenized 2160000 documents in 105.49314051469167 minutes\n",
      "Tokenized 2170000 documents in 105.96978049675623 minutes\n",
      "Tokenized 2180000 documents in 106.43757760922114 minutes\n",
      "Tokenized 2190000 documents in 106.92256632248561 minutes\n",
      "Tokenized 2200000 documents in 107.40721499919891 minutes\n",
      "Tokenized 2210000 documents in 107.90410601695379 minutes\n",
      "Tokenized 2220000 documents in 108.41005308628083 minutes\n",
      "Tokenized 2230000 documents in 108.90737985372543 minutes\n",
      "Tokenized 2240000 documents in 109.3891643166542 minutes\n",
      "Tokenized 2250000 documents in 109.87010918458303 minutes\n",
      "Tokenized 2260000 documents in 110.34448495308558 minutes\n",
      "Tokenized 2270000 documents in 110.82146226962408 minutes\n",
      "Tokenized 2280000 documents in 111.28368471463521 minutes\n",
      "Tokenized 2290000 documents in 111.75996528069179 minutes\n",
      "Tokenized 2300000 documents in 112.25922772487004 minutes\n",
      "Tokenized 2310000 documents in 112.73812669118246 minutes\n",
      "Tokenized 2320000 documents in 113.21834944883982 minutes\n",
      "Tokenized 2330000 documents in 113.69577485322952 minutes\n",
      "Tokenized 2340000 documents in 114.17453900973003 minutes\n",
      "Tokenized 2350000 documents in 114.64048213561377 minutes\n",
      "Tokenized 2360000 documents in 115.117966679732 minutes\n",
      "Tokenized 2370000 documents in 115.59125099976858 minutes\n",
      "Tokenized 2380000 documents in 116.1571170171102 minutes\n",
      "Tokenized 2390000 documents in 116.64433745543163 minutes\n",
      "Tokenized 2400000 documents in 117.13123555183411 minutes\n",
      "Tokenized 2410000 documents in 117.61246167023977 minutes\n",
      "Tokenized 2420000 documents in 118.11111279328664 minutes\n",
      "Tokenized 2430000 documents in 118.59359725316365 minutes\n",
      "Tokenized 2440000 documents in 119.08045341968537 minutes\n",
      "Tokenized 2450000 documents in 119.56959520578384 minutes\n",
      "Tokenized 2460000 documents in 120.05952316125234 minutes\n",
      "Tokenized 2470000 documents in 120.54818978309632 minutes\n",
      "Tokenized 2480000 documents in 121.03754897912343 minutes\n",
      "Tokenized 2490000 documents in 121.52228470245997 minutes\n",
      "Tokenized 2500000 documents in 122.00832327206929 minutes\n",
      "Tokenized 2510000 documents in 122.5086860259374 minutes\n",
      "Tokenized 2520000 documents in 122.98686743577322 minutes\n",
      "Tokenized 2530000 documents in 123.46329409678778 minutes\n",
      "Tokenized 2540000 documents in 123.95508035818736 minutes\n",
      "Tokenized 2550000 documents in 124.44553437630336 minutes\n",
      "Tokenized 2560000 documents in 124.93995139996211 minutes\n",
      "Tokenized 2570000 documents in 125.44831297000249 minutes\n",
      "Tokenized 2580000 documents in 125.94797389904657 minutes\n",
      "Tokenized 2590000 documents in 126.43668829600016 minutes\n",
      "Tokenized 2600000 documents in 126.91675006945928 minutes\n",
      "Tokenized 2610000 documents in 127.40322134494781 minutes\n",
      "Tokenized 2620000 documents in 127.88767468134562 minutes\n",
      "Tokenized 2630000 documents in 128.3572268605232 minutes\n",
      "Tokenized 2640000 documents in 128.83964227040607 minutes\n",
      "Tokenized 2650000 documents in 129.33530296881995 minutes\n",
      "Tokenized 2660000 documents in 129.80995423793792 minutes\n",
      "Tokenized 2670000 documents in 130.28240866263707 minutes\n",
      "Tokenized 2680000 documents in 130.74726585149764 minutes\n",
      "Tokenized 2690000 documents in 131.21756418546042 minutes\n",
      "Tokenized 2700000 documents in 131.67382243871688 minutes\n",
      "Tokenized 2710000 documents in 132.14627603292465 minutes\n",
      "Tokenized 2720000 documents in 132.60937857230505 minutes\n",
      "Tokenized 2730000 documents in 133.08680590788524 minutes\n",
      "Tokenized 2740000 documents in 133.56791225274404 minutes\n",
      "Tokenized 2750000 documents in 134.04876800378165 minutes\n",
      "Tokenized 2760000 documents in 134.53028312921523 minutes\n",
      "Tokenized 2770000 documents in 135.02020751635234 minutes\n",
      "Tokenized 2780000 documents in 135.49383070866267 minutes\n",
      "Tokenized 2790000 documents in 135.98456780910493 minutes\n",
      "Tokenized 2800000 documents in 136.46901157697042 minutes\n",
      "Tokenized 2810000 documents in 136.958227622509 minutes\n",
      "Tokenized 2820000 documents in 137.4428429166476 minutes\n",
      "Tokenized 2830000 documents in 137.92633692026138 minutes\n",
      "Tokenized 2840000 documents in 138.41270714203517 minutes\n",
      "Tokenized 2850000 documents in 138.8823146343231 minutes\n",
      "Tokenized 2860000 documents in 139.37816925048827 minutes\n",
      "Tokenized 2870000 documents in 139.8578650712967 minutes\n",
      "Tokenized 2880000 documents in 140.3256512562434 minutes\n",
      "Tokenized 2890000 documents in 140.81327169736227 minutes\n",
      "Tokenized 2900000 documents in 141.30168942213058 minutes\n",
      "Tokenized 2910000 documents in 141.78992134332657 minutes\n",
      "Tokenized 2920000 documents in 142.29665548006693 minutes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized 2930000 documents in 142.78816661039988 minutes\n",
      "Tokenized 2940000 documents in 143.28276998202006 minutes\n",
      "Tokenized 2950000 documents in 143.75757474104563 minutes\n",
      "Tokenized 2960000 documents in 144.24034826358158 minutes\n",
      "Tokenized 2970000 documents in 144.7172559817632 minutes\n",
      "Tokenized 2980000 documents in 145.19437002340953 minutes\n"
     ]
    }
   ],
   "source": [
    "filtered_tokens = []\n",
    "start = time.time()\n",
    "i = 1\n",
    "for doc in nlp.pipe(docs, disable=['tagger', 'parser', 'ner', 'textcat'], batch_size=10000):\n",
    "    try:\n",
    "        tokens = [token.lemma_ for token in doc if token_filter(token)]\n",
    "        filtered_tokens.append(tokens)\n",
    "        i += 1\n",
    "        if i % 10000 == 0:\n",
    "            print(f'Tokenized {i} documents in {(time.time()-start)/60} minutes')\n",
    "    except:\n",
    "        print(f'Document {i} has an encoding error/has error characters.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the tokenized restaurant reviews to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/tokenized_rest_reviews.pkl', 'wb') as f:\n",
    "    pickle.dump(filtered_tokens, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = businesses['processed'].astype('str').astype('unicode').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized 10000 documents in 0.5455632209777832 minutes\n",
      "Tokenized 20000 documents in 1.1006454229354858 minutes\n",
      "Tokenized 30000 documents in 1.7564432183901468 minutes\n",
      "Tokenized 40000 documents in 2.3085965077082315 minutes\n",
      "Tokenized 50000 documents in 2.8641520818074544 minutes\n",
      "Tokenized 60000 documents in 3.405859084924062 minutes\n",
      "Tokenized 70000 documents in 3.9494373003641763 minutes\n",
      "Tokenized 80000 documents in 4.483182958761851 minutes\n",
      "Tokenized 90000 documents in 5.038688929875692 minutes\n",
      "Tokenized 100000 documents in 5.60417918364207 minutes\n",
      "Tokenized 110000 documents in 6.160481135050456 minutes\n",
      "Tokenized 120000 documents in 6.709739796320597 minutes\n",
      "Tokenized 130000 documents in 7.259782950083415 minutes\n",
      "Tokenized 140000 documents in 7.803244296709696 minutes\n",
      "Tokenized 150000 documents in 8.351007922490437 minutes\n",
      "Tokenized 160000 documents in 8.897802464167277 minutes\n",
      "Tokenized 170000 documents in 9.447558689117432 minutes\n",
      "Tokenized 180000 documents in 9.995304199059804 minutes\n",
      "Tokenized 190000 documents in 10.539700543880462 minutes\n",
      "Tokenized 200000 documents in 11.096505836645763 minutes\n",
      "Tokenized 210000 documents in 11.634421785672506 minutes\n",
      "Tokenized 220000 documents in 12.177930680910746 minutes\n",
      "Tokenized 230000 documents in 12.720233396689098 minutes\n",
      "Tokenized 240000 documents in 13.277133770783742 minutes\n",
      "Tokenized 250000 documents in 13.841665450731913 minutes\n",
      "Tokenized 260000 documents in 14.385176332791646 minutes\n",
      "Tokenized 270000 documents in 14.930123794078828 minutes\n",
      "Tokenized 280000 documents in 15.490901549657186 minutes\n",
      "Tokenized 290000 documents in 16.039862473805744 minutes\n",
      "Tokenized 300000 documents in 16.585318557421367 minutes\n",
      "Tokenized 310000 documents in 17.11304655869802 minutes\n",
      "Tokenized 320000 documents in 17.61543710231781 minutes\n",
      "Tokenized 330000 documents in 18.128424183527628 minutes\n",
      "Tokenized 340000 documents in 18.628407414754232 minutes\n",
      "Tokenized 350000 documents in 19.118134021759033 minutes\n",
      "Tokenized 360000 documents in 19.611695206165315 minutes\n",
      "Tokenized 370000 documents in 20.108043118317923 minutes\n",
      "Tokenized 380000 documents in 20.607648106416068 minutes\n",
      "Tokenized 390000 documents in 21.111082899570466 minutes\n",
      "Tokenized 400000 documents in 21.60440639257431 minutes\n",
      "Tokenized 410000 documents in 22.105876580874124 minutes\n",
      "Tokenized 420000 documents in 22.615454081694285 minutes\n",
      "Tokenized 430000 documents in 23.11779820919037 minutes\n",
      "Tokenized 440000 documents in 23.62390285730362 minutes\n",
      "Tokenized 450000 documents in 24.126884786287942 minutes\n",
      "Tokenized 460000 documents in 24.636904219786327 minutes\n",
      "Tokenized 470000 documents in 25.129719932874043 minutes\n",
      "Tokenized 480000 documents in 25.637656931082407 minutes\n",
      "Tokenized 490000 documents in 26.1344250758489 minutes\n",
      "Tokenized 500000 documents in 26.63630970319112 minutes\n",
      "Tokenized 510000 documents in 27.140299797058105 minutes\n",
      "Tokenized 520000 documents in 27.641212582588196 minutes\n",
      "Tokenized 530000 documents in 28.144425674279532 minutes\n",
      "Tokenized 540000 documents in 28.633346819877623 minutes\n",
      "Tokenized 550000 documents in 29.12743835846583 minutes\n",
      "Tokenized 560000 documents in 29.634231448173523 minutes\n",
      "Tokenized 570000 documents in 30.13807638088862 minutes\n",
      "Tokenized 580000 documents in 30.63453185558319 minutes\n",
      "Tokenized 590000 documents in 31.129743786652885 minutes\n",
      "Tokenized 600000 documents in 31.622279226779938 minutes\n",
      "Tokenized 610000 documents in 32.123535720507306 minutes\n",
      "Tokenized 620000 documents in 32.63632949590683 minutes\n",
      "Tokenized 630000 documents in 33.14145270188649 minutes\n",
      "Tokenized 640000 documents in 33.64500158230464 minutes\n",
      "Tokenized 650000 documents in 34.15432997941971 minutes\n",
      "Tokenized 660000 documents in 34.65324372450511 minutes\n",
      "Tokenized 670000 documents in 35.16846460103989 minutes\n",
      "Tokenized 680000 documents in 35.676772713661194 minutes\n",
      "Tokenized 690000 documents in 36.17894680102666 minutes\n",
      "Tokenized 700000 documents in 36.68370225429535 minutes\n",
      "Tokenized 710000 documents in 37.18318742116292 minutes\n",
      "Tokenized 720000 documents in 37.69734710454941 minutes\n",
      "Tokenized 730000 documents in 38.20299599170685 minutes\n",
      "Tokenized 740000 documents in 38.71241191625595 minutes\n",
      "Tokenized 750000 documents in 39.22994846900304 minutes\n",
      "Tokenized 760000 documents in 39.73647511402766 minutes\n",
      "Tokenized 770000 documents in 40.24926573435466 minutes\n",
      "Tokenized 780000 documents in 40.753206018606825 minutes\n",
      "Tokenized 790000 documents in 41.264734439055125 minutes\n",
      "Tokenized 800000 documents in 41.77532213926315 minutes\n",
      "Tokenized 810000 documents in 42.28299424648285 minutes\n",
      "Tokenized 820000 documents in 42.797337329387666 minutes\n",
      "Tokenized 830000 documents in 43.44130742549896 minutes\n",
      "Tokenized 840000 documents in 43.94712405204773 minutes\n",
      "Tokenized 850000 documents in 44.44943896532059 minutes\n",
      "Tokenized 860000 documents in 44.966703379154204 minutes\n",
      "Tokenized 870000 documents in 45.478275219599404 minutes\n",
      "Tokenized 880000 documents in 45.974750284353895 minutes\n",
      "Tokenized 890000 documents in 46.488932116826376 minutes\n",
      "Tokenized 900000 documents in 46.99247382481893 minutes\n",
      "Tokenized 910000 documents in 47.510190057754514 minutes\n",
      "Tokenized 920000 documents in 48.021723632017775 minutes\n",
      "Tokenized 930000 documents in 48.52909531990687 minutes\n",
      "Tokenized 940000 documents in 49.03333038489024 minutes\n",
      "Tokenized 950000 documents in 49.548418227831526 minutes\n",
      "Tokenized 960000 documents in 50.05338396231333 minutes\n",
      "Tokenized 970000 documents in 50.57595809698105 minutes\n",
      "Tokenized 980000 documents in 51.10241821606954 minutes\n",
      "Tokenized 990000 documents in 51.60603900353114 minutes\n",
      "Tokenized 1000000 documents in 52.119980959097546 minutes\n",
      "Tokenized 1010000 documents in 52.62521915038427 minutes\n",
      "Tokenized 1020000 documents in 53.12648618221283 minutes\n",
      "Tokenized 1030000 documents in 53.638980448246 minutes\n",
      "Tokenized 1040000 documents in 54.1567364970843 minutes\n",
      "Tokenized 1050000 documents in 54.66037996212641 minutes\n",
      "Tokenized 1060000 documents in 55.1498703678449 minutes\n",
      "Tokenized 1070000 documents in 55.64288069009781 minutes\n",
      "Tokenized 1080000 documents in 56.146314811706546 minutes\n",
      "Tokenized 1090000 documents in 56.65926800171534 minutes\n",
      "Tokenized 1100000 documents in 57.16205981175105 minutes\n",
      "Tokenized 1110000 documents in 57.67073339223862 minutes\n",
      "Tokenized 1120000 documents in 58.1845107515653 minutes\n",
      "Tokenized 1130000 documents in 58.689435215791065 minutes\n",
      "Tokenized 1140000 documents in 59.20129566192627 minutes\n",
      "Tokenized 1150000 documents in 59.72003703912099 minutes\n",
      "Tokenized 1160000 documents in 60.22240058581034 minutes\n",
      "Tokenized 1170000 documents in 60.73827386697133 minutes\n",
      "Tokenized 1180000 documents in 61.244372777144115 minutes\n",
      "Tokenized 1190000 documents in 61.75927971204122 minutes\n",
      "Tokenized 1200000 documents in 62.26549533605576 minutes\n",
      "Tokenized 1210000 documents in 62.77214500506719 minutes\n",
      "Tokenized 1220000 documents in 63.27518878777822 minutes\n",
      "Tokenized 1230000 documents in 63.78587424357732 minutes\n",
      "Tokenized 1240000 documents in 64.29843240976334 minutes\n",
      "Tokenized 1250000 documents in 64.80251057942708 minutes\n",
      "Tokenized 1260000 documents in 65.31610996325811 minutes\n",
      "Tokenized 1270000 documents in 65.83572560946146 minutes\n",
      "Tokenized 1280000 documents in 66.3384716073672 minutes\n",
      "Tokenized 1290000 documents in 66.84026083946227 minutes\n",
      "Tokenized 1300000 documents in 67.34777757724126 minutes\n",
      "Tokenized 1310000 documents in 67.86463813384374 minutes\n",
      "Tokenized 1320000 documents in 68.38198023637136 minutes\n",
      "Tokenized 1330000 documents in 68.89696696996688 minutes\n",
      "Tokenized 1340000 documents in 69.40790536403657 minutes\n",
      "Tokenized 1350000 documents in 69.91831769545873 minutes\n",
      "Tokenized 1360000 documents in 70.4296785791715 minutes\n",
      "Tokenized 1370000 documents in 70.94683587948481 minutes\n",
      "Tokenized 1380000 documents in 71.47029386758804 minutes\n",
      "Tokenized 1390000 documents in 71.96905936002732 minutes\n",
      "Tokenized 1400000 documents in 72.47336247762044 minutes\n",
      "Tokenized 1410000 documents in 72.97437554597855 minutes\n",
      "Tokenized 1420000 documents in 73.48043620586395 minutes\n"
     ]
    }
   ],
   "source": [
    "filtered_tokens_bs = []\n",
    "start = time.time()\n",
    "i = 1\n",
    "for doc in nlp.pipe(docs, disable=['tagger', 'parser', 'ner', 'textcat'], batch_size=10000):\n",
    "    try:\n",
    "        tokens = [token.lemma_ for token in doc if token_filter(token)]\n",
    "        filtered_tokens_bs.append(tokens)\n",
    "        i += 1\n",
    "        if i % 10000 == 0:\n",
    "            print(f'Tokenized {i} documents in {(time.time()-start)/60} minutes')\n",
    "    except:\n",
    "        print(f'Document {i} has an encoding error.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/tokenized_bs_reviews.pkl', 'wb') as f:\n",
    "    pickle.dump(filtered_tokens_bs, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
