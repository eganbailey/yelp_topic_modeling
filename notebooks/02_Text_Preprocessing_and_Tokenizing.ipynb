{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "In this notebook, we process and tokenize each review text. Tokenization transforms each review into a list of relevant tokens; these reviews are added to a corpus as a list. Since restaurants dominate the Yelp review space, two corpuses are developed: one for restaurants, and one for all other types of businesses.\n",
    "\n",
    "## Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "import textacy\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "engine = create_engine('postgres://postgres:moop@52.25.218.143:5432/capstone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data from server\n",
    "\n",
    "### Query for Restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3221418, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = '''SELECT reviews.text as text,\n",
    "reviews.useful as useful,\n",
    "business.categories as categories\n",
    "FROM reviews\n",
    "INNER JOIN business ON reviews.business_id = business.business_id\n",
    "WHERE categories LIKE '%%Restaurants%%'\n",
    "'''\n",
    "\n",
    "restaurants = pd.read_sql_query(query, engine)\n",
    "restaurants.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query for Other Businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2040250, 5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = '''\n",
    "SELECT \n",
    "reviews.text as text, \n",
    "reviews.useful as useful, \n",
    "reviews.funny as funny, \n",
    "reviews.cool as cool,\n",
    "business.categories as categories\n",
    "FROM reviews\n",
    "INNER JOIN business ON reviews.business_id = business.business_id\n",
    "WHERE categories NOT LIKE '%%Restaurants%%'\n",
    "'''\n",
    "\n",
    "businesses = pd.read_sql_query(query, engine)\n",
    "businesses.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the text\n",
    "\n",
    "I use textacy's preprocess method to convert all the text to lowercase and remove numbers, URLs, and punctuation and save it to a file so we can load it in to work on.\n",
    "\n",
    "### Restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants.drop('categories', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24min 49s, sys: 0 ns, total: 24min 49s\n",
      "Wall time: 24min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "restaurants['processed'] = restaurants['text'].map(lambda x: textacy.preprocess.preprocess_text(x, lowercase=True, no_urls=True, no_punct=True, no_numbers=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants.drop('text', 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants.to_csv('../data/rests.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cycle Pub Las Vegas was a blast! Got a groupon...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Pubs;Bars;Bar Crawl;Tours;Nightlife;Hotels &amp; T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I thought Tidy's Flowers had a great reputatio...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Event Planning &amp; Services;Flowers &amp; Gifts;Flor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I too have been trying to book an appt to use ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Beauty &amp; Spas;Massage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Great place to bring dogs! It's really a dog p...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Pet Services;Pets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Decided to give this place a try since I was i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Food;Bakeries</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  useful  funny  cool  \\\n",
       "0  Cycle Pub Las Vegas was a blast! Got a groupon...       1      0     0   \n",
       "1  I thought Tidy's Flowers had a great reputatio...       9      0     1   \n",
       "2  I too have been trying to book an appt to use ...       0      0     0   \n",
       "3  Great place to bring dogs! It's really a dog p...       0      0     0   \n",
       "4  Decided to give this place a try since I was i...       0      0     0   \n",
       "\n",
       "                                          categories  \n",
       "0  Pubs;Bars;Bar Crawl;Tours;Nightlife;Hotels & T...  \n",
       "1  Event Planning & Services;Flowers & Gifts;Flor...  \n",
       "2                              Beauty & Spas;Massage  \n",
       "3                                  Pet Services;Pets  \n",
       "4                                      Food;Bakeries  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "businesses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "businesses.drop('categories', 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16min 38s, sys: 0 ns, total: 16min 38s\n",
      "Wall time: 16min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "businesses['processed'] = businesses['text'].map(lambda x: textacy.preprocess.preprocess_text(x, lowercase=True, no_urls=True, no_punct=True, no_numbers=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "businesses.drop('text', 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "businesses.to_csv('../data/businesses.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing\n",
    "\n",
    "In this section I set up a spacy tokenizer. We disable part-of-speech tagging, semantic parsing, and text categorization to reduce overall memory usage. We also create a filter function to eliminate stopwords and short tokens (less than 4 characters) from our final tokenized documents. The tokenized documents are then added to a list which we can pass through a vectorizer (see Notebook 3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants = pd.read_csv('../data/rests.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = textacy.load_spacy(\"en_core_web_sm\", disable = (\"tagger\", \"parser\", \"ner\", \"textcat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_filter(token): #remove stopwords and tokens 4 char or less\n",
    "    return not (token.is_stop | len(token.text) <= 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = restaurants['processed'].astype('unicode').tolist() # The spacy pipeline requires raw text to be in utf-8 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized 10000 documents in 0.5417954802513123 minutes\n",
      "Tokenized 20000 documents in 1.0150205810864767 minutes\n",
      "Tokenized 30000 documents in 1.4781464775403341 minutes\n",
      "Tokenized 40000 documents in 1.9605997800827026 minutes\n",
      "Tokenized 50000 documents in 2.440075139204661 minutes\n",
      "Tokenized 60000 documents in 2.9163665135701495 minutes\n",
      "Tokenized 70000 documents in 3.4392976482709248 minutes\n",
      "Tokenized 80000 documents in 3.963463226954142 minutes\n",
      "Tokenized 90000 documents in 4.478336413701375 minutes\n",
      "Tokenized 100000 documents in 4.979599559307099 minutes\n",
      "Tokenized 110000 documents in 5.450708782672882 minutes\n",
      "Tokenized 120000 documents in 5.970176621278127 minutes\n",
      "Tokenized 130000 documents in 6.496784055233002 minutes\n",
      "Tokenized 140000 documents in 7.020404386520386 minutes\n",
      "Tokenized 150000 documents in 7.524508945147196 minutes\n",
      "Tokenized 160000 documents in 8.063184555371603 minutes\n",
      "Tokenized 170000 documents in 8.576491578420002 minutes\n",
      "Tokenized 180000 documents in 9.10695337454478 minutes\n",
      "Tokenized 190000 documents in 9.64609891573588 minutes\n",
      "Tokenized 200000 documents in 10.206779984633128 minutes\n",
      "Tokenized 210000 documents in 10.726046947638194 minutes\n",
      "Tokenized 220000 documents in 11.232485262552897 minutes\n",
      "Tokenized 230000 documents in 11.752638252576192 minutes\n",
      "Tokenized 240000 documents in 12.270227158069611 minutes\n",
      "Tokenized 250000 documents in 12.798421963055928 minutes\n",
      "Tokenized 260000 documents in 13.33028830687205 minutes\n",
      "Tokenized 270000 documents in 13.854759617646534 minutes\n",
      "Tokenized 280000 documents in 14.337329777081807 minutes\n",
      "Tokenized 290000 documents in 14.79949899117152 minutes\n",
      "Tokenized 300000 documents in 15.277323615550994 minutes\n",
      "Tokenized 310000 documents in 15.754707252979278 minutes\n",
      "Tokenized 320000 documents in 16.221837516625722 minutes\n",
      "Tokenized 330000 documents in 16.683766961097717 minutes\n",
      "Tokenized 340000 documents in 17.155853275458018 minutes\n",
      "Tokenized 350000 documents in 17.624465227127075 minutes\n",
      "Tokenized 360000 documents in 18.09795287847519 minutes\n",
      "Tokenized 370000 documents in 18.585939788818358 minutes\n",
      "Tokenized 380000 documents in 19.07891273498535 minutes\n",
      "Tokenized 390000 documents in 19.56490752696991 minutes\n",
      "Tokenized 400000 documents in 20.07120614051819 minutes\n",
      "Tokenized 410000 documents in 20.53460196654002 minutes\n",
      "Tokenized 420000 documents in 21.01972187757492 minutes\n",
      "Tokenized 430000 documents in 21.49561270078023 minutes\n",
      "Tokenized 440000 documents in 21.993874808152515 minutes\n",
      "Tokenized 450000 documents in 22.48879305124283 minutes\n",
      "Tokenized 460000 documents in 22.979326931635537 minutes\n",
      "Tokenized 470000 documents in 23.465097200870513 minutes\n",
      "Tokenized 480000 documents in 23.941733102003735 minutes\n",
      "Tokenized 490000 documents in 24.43224969704946 minutes\n",
      "Tokenized 500000 documents in 24.92488729953766 minutes\n",
      "Tokenized 510000 documents in 25.416098459561667 minutes\n",
      "Tokenized 520000 documents in 25.911874135335285 minutes\n",
      "Tokenized 530000 documents in 26.407555862267813 minutes\n",
      "Tokenized 540000 documents in 26.89617376724879 minutes\n",
      "Tokenized 550000 documents in 27.401676193873087 minutes\n",
      "Tokenized 560000 documents in 27.887692618370057 minutes\n",
      "Tokenized 570000 documents in 28.3572438955307 minutes\n",
      "Tokenized 580000 documents in 28.845180149873098 minutes\n",
      "Tokenized 590000 documents in 29.32047731479009 minutes\n",
      "Tokenized 600000 documents in 29.802535208066306 minutes\n",
      "Tokenized 610000 documents in 30.30286478996277 minutes\n",
      "Tokenized 620000 documents in 30.78116666475932 minutes\n",
      "Tokenized 630000 documents in 31.280754188696545 minutes\n",
      "Tokenized 640000 documents in 31.77744370698929 minutes\n",
      "Tokenized 650000 documents in 32.29440863529841 minutes\n",
      "Tokenized 660000 documents in 32.82456565697988 minutes\n",
      "Tokenized 670000 documents in 33.32913777033488 minutes\n",
      "Tokenized 680000 documents in 33.82996357679367 minutes\n",
      "Tokenized 690000 documents in 34.30610681772232 minutes\n",
      "Tokenized 700000 documents in 34.77553664048513 minutes\n",
      "Tokenized 710000 documents in 35.25963828166326 minutes\n",
      "Tokenized 720000 documents in 35.744809635480244 minutes\n",
      "Tokenized 730000 documents in 36.25791986783346 minutes\n",
      "Tokenized 740000 documents in 36.739539595444995 minutes\n",
      "Tokenized 750000 documents in 37.236069611708324 minutes\n",
      "Tokenized 760000 documents in 37.74873016277949 minutes\n",
      "Tokenized 770000 documents in 38.233199314276376 minutes\n",
      "Tokenized 780000 documents in 38.719994298617046 minutes\n",
      "Tokenized 790000 documents in 39.23294449249904 minutes\n",
      "Tokenized 800000 documents in 39.712914220492046 minutes\n",
      "Tokenized 810000 documents in 40.20989644527435 minutes\n",
      "Tokenized 820000 documents in 40.711837502320606 minutes\n",
      "Tokenized 830000 documents in 41.18748462994893 minutes\n",
      "Tokenized 840000 documents in 41.67466759681702 minutes\n",
      "Tokenized 850000 documents in 42.168212393919625 minutes\n",
      "Tokenized 860000 documents in 42.661025468508406 minutes\n",
      "Tokenized 870000 documents in 43.14427484671275 minutes\n",
      "Tokenized 880000 documents in 43.653994750976565 minutes\n",
      "Tokenized 890000 documents in 44.11711752812068 minutes\n",
      "Tokenized 900000 documents in 44.59048976500829 minutes\n",
      "Tokenized 910000 documents in 45.083402796586356 minutes\n",
      "Tokenized 920000 documents in 45.55418865680694 minutes\n",
      "Tokenized 930000 documents in 46.05278566281001 minutes\n",
      "Tokenized 940000 documents in 46.575605964660646 minutes\n",
      "Tokenized 950000 documents in 47.075642029444374 minutes\n",
      "Tokenized 960000 documents in 47.594382019837695 minutes\n",
      "Tokenized 970000 documents in 48.11898903052012 minutes\n",
      "Tokenized 980000 documents in 48.641314586003624 minutes\n",
      "Tokenized 990000 documents in 49.213129460811615 minutes\n",
      "Tokenized 1000000 documents in 49.73682837486267 minutes\n",
      "Tokenized 1010000 documents in 50.25277552207311 minutes\n",
      "Tokenized 1020000 documents in 50.767513477802275 minutes\n",
      "Tokenized 1030000 documents in 51.290363264083865 minutes\n",
      "Tokenized 1040000 documents in 51.81084963480632 minutes\n",
      "Tokenized 1050000 documents in 52.38289101521174 minutes\n",
      "Tokenized 1060000 documents in 52.98577927748362 minutes\n",
      "Tokenized 1070000 documents in 53.55958652496338 minutes\n",
      "Tokenized 1080000 documents in 54.14278272390366 minutes\n",
      "Tokenized 1090000 documents in 54.7267782330513 minutes\n",
      "Tokenized 1100000 documents in 55.29669978221258 minutes\n",
      "Tokenized 1110000 documents in 55.838801232973736 minutes\n",
      "Tokenized 1120000 documents in 56.38251832326253 minutes\n",
      "Tokenized 1130000 documents in 56.93024601538976 minutes\n",
      "Tokenized 1140000 documents in 57.477346237500505 minutes\n",
      "Tokenized 1150000 documents in 57.99674481550853 minutes\n",
      "Tokenized 1160000 documents in 58.55161833365758 minutes\n",
      "Tokenized 1170000 documents in 59.09626183509827 minutes\n",
      "Tokenized 1180000 documents in 59.636367793877916 minutes\n",
      "Tokenized 1190000 documents in 60.13726760546366 minutes\n",
      "Tokenized 1200000 documents in 60.68227477868398 minutes\n",
      "Tokenized 1210000 documents in 61.214014514287314 minutes\n",
      "Tokenized 1220000 documents in 61.76553151210149 minutes\n",
      "Tokenized 1230000 documents in 62.29207376639048 minutes\n",
      "Tokenized 1240000 documents in 62.83405106862386 minutes\n",
      "Tokenized 1250000 documents in 63.35835103591283 minutes\n",
      "Tokenized 1260000 documents in 63.878066098690034 minutes\n",
      "Tokenized 1270000 documents in 64.39100184043248 minutes\n",
      "Tokenized 1280000 documents in 64.9225351413091 minutes\n",
      "Tokenized 1290000 documents in 65.42574622631074 minutes\n",
      "Tokenized 1300000 documents in 65.98554470936458 minutes\n",
      "Tokenized 1310000 documents in 66.55910729964575 minutes\n",
      "Tokenized 1320000 documents in 67.11015702088675 minutes\n",
      "Tokenized 1330000 documents in 67.6517034928004 minutes\n",
      "Tokenized 1340000 documents in 68.21643268267313 minutes\n",
      "Tokenized 1350000 documents in 68.787575785319 minutes\n",
      "Tokenized 1360000 documents in 69.32464109659195 minutes\n",
      "Tokenized 1370000 documents in 69.8708770831426 minutes\n",
      "Tokenized 1380000 documents in 70.4356223543485 minutes\n",
      "Tokenized 1390000 documents in 70.968423863252 minutes\n",
      "Tokenized 1400000 documents in 71.55673625071843 minutes\n",
      "Tokenized 1410000 documents in 72.07416739463807 minutes\n",
      "Tokenized 1420000 documents in 72.58494481245677 minutes\n",
      "Tokenized 1430000 documents in 73.14764609734218 minutes\n",
      "Tokenized 1440000 documents in 73.70361497004826 minutes\n",
      "Tokenized 1450000 documents in 74.23204393386841 minutes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized 1460000 documents in 74.72361800670623 minutes\n",
      "Tokenized 1470000 documents in 75.24632306098938 minutes\n",
      "Tokenized 1480000 documents in 75.76393938064575 minutes\n",
      "Tokenized 1490000 documents in 76.2843542178472 minutes\n",
      "Tokenized 1500000 documents in 76.79368238051732 minutes\n",
      "Tokenized 1510000 documents in 77.30218292077383 minutes\n",
      "Tokenized 1520000 documents in 77.81592822472254 minutes\n",
      "Tokenized 1530000 documents in 78.32706807454427 minutes\n",
      "Tokenized 1540000 documents in 78.8543143471082 minutes\n",
      "Tokenized 1550000 documents in 79.36111665169398 minutes\n",
      "Tokenized 1560000 documents in 79.8617068608602 minutes\n",
      "Tokenized 1570000 documents in 80.35899256865183 minutes\n",
      "Tokenized 1580000 documents in 80.87297044992447 minutes\n",
      "Tokenized 1590000 documents in 81.38849177360535 minutes\n",
      "Tokenized 1600000 documents in 81.92208697001139 minutes\n",
      "Tokenized 1610000 documents in 82.44450322389602 minutes\n",
      "Tokenized 1620000 documents in 82.9399381160736 minutes\n",
      "Tokenized 1630000 documents in 83.45261228879293 minutes\n",
      "Tokenized 1640000 documents in 83.97093226114909 minutes\n",
      "Tokenized 1650000 documents in 84.48311646779378 minutes\n",
      "Tokenized 1660000 documents in 84.9989589770635 minutes\n",
      "Tokenized 1670000 documents in 85.50215214888254 minutes\n",
      "Tokenized 1680000 documents in 85.9601268808047 minutes\n",
      "Tokenized 1690000 documents in 86.4991593003273 minutes\n",
      "Tokenized 1700000 documents in 86.960047463576 minutes\n",
      "Tokenized 1710000 documents in 87.41039567391077 minutes\n",
      "Tokenized 1720000 documents in 87.87417642672857 minutes\n",
      "Tokenized 1730000 documents in 88.33537431160609 minutes\n",
      "Tokenized 1740000 documents in 88.81223380963007 minutes\n",
      "Tokenized 1750000 documents in 89.33388170003892 minutes\n",
      "Tokenized 1760000 documents in 89.82943232059479 minutes\n",
      "Tokenized 1770000 documents in 90.29884337584177 minutes\n",
      "Tokenized 1780000 documents in 90.75297316710154 minutes\n",
      "Tokenized 1790000 documents in 91.21204978624979 minutes\n",
      "Tokenized 1800000 documents in 91.70899042288463 minutes\n",
      "Tokenized 1810000 documents in 92.16413327058156 minutes\n",
      "Tokenized 1820000 documents in 92.63558724721273 minutes\n",
      "Tokenized 1830000 documents in 93.11199474732081 minutes\n",
      "Tokenized 1840000 documents in 93.57763079007466 minutes\n",
      "Tokenized 1850000 documents in 94.04740487734476 minutes\n",
      "Tokenized 1860000 documents in 94.52827262878418 minutes\n",
      "Tokenized 1870000 documents in 95.02976791858673 minutes\n",
      "Tokenized 1880000 documents in 95.49066147406896 minutes\n",
      "Tokenized 1890000 documents in 95.9539122502009 minutes\n",
      "Tokenized 1900000 documents in 96.43548213640848 minutes\n",
      "Tokenized 1910000 documents in 96.94852888584137 minutes\n",
      "Tokenized 1920000 documents in 97.41181911230088 minutes\n",
      "Tokenized 1930000 documents in 97.88875372409821 minutes\n",
      "Tokenized 1940000 documents in 98.33718180656433 minutes\n",
      "Tokenized 1950000 documents in 98.78900499741236 minutes\n",
      "Tokenized 1960000 documents in 99.2479458530744 minutes\n",
      "Tokenized 1970000 documents in 99.70614807605743 minutes\n",
      "Tokenized 1980000 documents in 100.1506736477216 minutes\n",
      "Tokenized 1990000 documents in 100.59912431240082 minutes\n",
      "Tokenized 2000000 documents in 101.06790080865224 minutes\n",
      "Tokenized 2010000 documents in 101.54019351005554 minutes\n",
      "Tokenized 2020000 documents in 102.00965595642725 minutes\n",
      "Tokenized 2030000 documents in 102.49029091199239 minutes\n",
      "Tokenized 2040000 documents in 102.95792283614476 minutes\n",
      "Tokenized 2050000 documents in 103.43092132806778 minutes\n",
      "Tokenized 2060000 documents in 103.87542603413264 minutes\n",
      "Tokenized 2070000 documents in 104.35453469355902 minutes\n",
      "Tokenized 2080000 documents in 104.82781702280045 minutes\n",
      "Tokenized 2090000 documents in 105.32117046117783 minutes\n",
      "Tokenized 2100000 documents in 105.81298068364461 minutes\n",
      "Tokenized 2110000 documents in 106.30906665325165 minutes\n",
      "Tokenized 2120000 documents in 106.77979956467946 minutes\n",
      "Tokenized 2130000 documents in 107.2632047812144 minutes\n",
      "Tokenized 2140000 documents in 107.74047694603603 minutes\n",
      "Tokenized 2150000 documents in 108.22274834712347 minutes\n",
      "Tokenized 2160000 documents in 108.71223032474518 minutes\n",
      "Tokenized 2170000 documents in 109.19716784556707 minutes\n",
      "Tokenized 2180000 documents in 109.68236037890117 minutes\n",
      "Tokenized 2190000 documents in 110.1839370449384 minutes\n",
      "Tokenized 2200000 documents in 110.64810220797857 minutes\n",
      "Tokenized 2210000 documents in 111.12350496848424 minutes\n",
      "Tokenized 2220000 documents in 111.59114046494166 minutes\n",
      "Tokenized 2230000 documents in 112.07039646704992 minutes\n",
      "Tokenized 2240000 documents in 112.54709975719452 minutes\n",
      "Tokenized 2250000 documents in 113.01775912046432 minutes\n",
      "Tokenized 2260000 documents in 113.50955983400345 minutes\n",
      "Tokenized 2270000 documents in 113.9936559398969 minutes\n",
      "Tokenized 2280000 documents in 114.49696929454804 minutes\n",
      "Tokenized 2290000 documents in 114.9906017700831 minutes\n",
      "Tokenized 2300000 documents in 115.46316648721695 minutes\n",
      "Tokenized 2310000 documents in 115.93048200607299 minutes\n",
      "Tokenized 2320000 documents in 116.38666882912318 minutes\n",
      "Tokenized 2330000 documents in 116.85834294954935 minutes\n",
      "Tokenized 2340000 documents in 117.33202894528706 minutes\n",
      "Tokenized 2350000 documents in 117.8336159825325 minutes\n",
      "Tokenized 2360000 documents in 118.31880412896474 minutes\n",
      "Tokenized 2370000 documents in 118.81275055011113 minutes\n",
      "Tokenized 2380000 documents in 119.32319300174713 minutes\n",
      "Tokenized 2390000 documents in 119.80539009571075 minutes\n",
      "Tokenized 2400000 documents in 120.29537176291147 minutes\n",
      "Tokenized 2410000 documents in 120.78947463830312 minutes\n",
      "Tokenized 2420000 documents in 121.25498529275258 minutes\n",
      "Tokenized 2430000 documents in 121.75476423104604 minutes\n",
      "Tokenized 2440000 documents in 122.22624415159225 minutes\n",
      "Tokenized 2450000 documents in 122.70168155829111 minutes\n",
      "Tokenized 2460000 documents in 123.1879679163297 minutes\n",
      "Tokenized 2470000 documents in 123.68244372606277 minutes\n",
      "Tokenized 2480000 documents in 124.16505748828253 minutes\n",
      "Tokenized 2490000 documents in 124.66341239611307 minutes\n",
      "Tokenized 2500000 documents in 125.1309657851855 minutes\n",
      "Tokenized 2510000 documents in 125.6085177342097 minutes\n",
      "Tokenized 2520000 documents in 126.0713470975558 minutes\n",
      "Tokenized 2530000 documents in 126.55966473023096 minutes\n",
      "Tokenized 2540000 documents in 127.06331108808517 minutes\n",
      "Tokenized 2550000 documents in 127.60107822815577 minutes\n",
      "Tokenized 2560000 documents in 128.08771246671677 minutes\n",
      "Tokenized 2570000 documents in 128.56591941515606 minutes\n",
      "Tokenized 2580000 documents in 129.05962651570638 minutes\n",
      "Tokenized 2590000 documents in 129.544726451238 minutes\n",
      "Tokenized 2600000 documents in 130.0119169553121 minutes\n",
      "Tokenized 2610000 documents in 130.47988497018815 minutes\n",
      "Tokenized 2620000 documents in 130.95570169289905 minutes\n",
      "Tokenized 2630000 documents in 131.42534700632095 minutes\n",
      "Tokenized 2640000 documents in 131.88776057958603 minutes\n",
      "Tokenized 2650000 documents in 132.39943130811056 minutes\n",
      "Tokenized 2660000 documents in 132.89576178391775 minutes\n",
      "Tokenized 2670000 documents in 133.3999927163124 minutes\n",
      "Tokenized 2680000 documents in 133.89449549118677 minutes\n",
      "Tokenized 2690000 documents in 134.37032627662023 minutes\n",
      "Tokenized 2700000 documents in 134.84358921051026 minutes\n",
      "Tokenized 2710000 documents in 135.31865485509238 minutes\n",
      "Tokenized 2720000 documents in 135.79611421028773 minutes\n",
      "Tokenized 2730000 documents in 136.2683233579 minutes\n",
      "Tokenized 2740000 documents in 136.7497904340426 minutes\n",
      "Tokenized 2750000 documents in 137.21716183423996 minutes\n",
      "Tokenized 2760000 documents in 137.69131764968236 minutes\n",
      "Tokenized 2770000 documents in 138.14783993959426 minutes\n",
      "Tokenized 2780000 documents in 138.62399160464605 minutes\n",
      "Tokenized 2790000 documents in 139.10408172210057 minutes\n",
      "Tokenized 2800000 documents in 139.59029991229374 minutes\n",
      "Tokenized 2810000 documents in 140.0544874628385 minutes\n",
      "Tokenized 2820000 documents in 140.54273337523142 minutes\n",
      "Tokenized 2830000 documents in 141.01710526148477 minutes\n",
      "Tokenized 2840000 documents in 141.49472552935282 minutes\n",
      "Tokenized 2850000 documents in 141.97472500403723 minutes\n",
      "Tokenized 2860000 documents in 142.45383389790854 minutes\n",
      "Tokenized 2870000 documents in 142.93638126452763 minutes\n",
      "Tokenized 2880000 documents in 143.43876669009526 minutes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized 2890000 documents in 143.91503901878994 minutes\n",
      "Tokenized 2900000 documents in 144.39802083969116 minutes\n",
      "Tokenized 2910000 documents in 144.9025940656662 minutes\n",
      "Tokenized 2920000 documents in 145.4133126695951 minutes\n",
      "Tokenized 2930000 documents in 145.89905375639597 minutes\n",
      "Tokenized 2940000 documents in 146.4042413433393 minutes\n",
      "Tokenized 2950000 documents in 146.90093118747075 minutes\n",
      "Tokenized 2960000 documents in 147.40726381540298 minutes\n",
      "Tokenized 2970000 documents in 147.8788391470909 minutes\n",
      "Tokenized 2980000 documents in 148.3371657927831 minutes\n",
      "Tokenized 2990000 documents in 148.870343708992 minutes\n",
      "Tokenized 3000000 documents in 149.38477727969487 minutes\n",
      "Tokenized 3010000 documents in 149.87994529008864 minutes\n",
      "Tokenized 3020000 documents in 150.3524611234665 minutes\n",
      "Tokenized 3030000 documents in 150.83511337836583 minutes\n",
      "Tokenized 3040000 documents in 151.3062119881312 minutes\n",
      "Tokenized 3050000 documents in 151.7816428343455 minutes\n",
      "Tokenized 3060000 documents in 152.25277065436046 minutes\n",
      "Tokenized 3070000 documents in 152.73873289028805 minutes\n",
      "Tokenized 3080000 documents in 153.2126100897789 minutes\n",
      "Tokenized 3090000 documents in 153.7061531861623 minutes\n",
      "Tokenized 3100000 documents in 154.16860611836117 minutes\n",
      "Tokenized 3110000 documents in 154.63655637105305 minutes\n",
      "Tokenized 3120000 documents in 155.09589665730795 minutes\n",
      "Tokenized 3130000 documents in 155.5704354484876 minutes\n",
      "Tokenized 3140000 documents in 156.05454599459966 minutes\n",
      "Tokenized 3150000 documents in 156.51422758102416 minutes\n",
      "Tokenized 3160000 documents in 157.009499168396 minutes\n",
      "Tokenized 3170000 documents in 157.467234369119 minutes\n",
      "Tokenized 3180000 documents in 157.95455903609593 minutes\n",
      "Tokenized 3190000 documents in 158.4205591360728 minutes\n",
      "Tokenized 3200000 documents in 158.90138441721598 minutes\n",
      "Tokenized 3210000 documents in 159.36122238636017 minutes\n",
      "Tokenized 3220000 documents in 159.79583954016368 minutes\n"
     ]
    }
   ],
   "source": [
    "filtered_tokens = []\n",
    "start = time.time()\n",
    "i = 1\n",
    "for doc in nlp.pipe(docs, disable=['tagger', 'parser', 'ner', 'textcat'], batch_size=10000):\n",
    "    tokens = [token.lemma_ for token in doc if token_filter(token)]\n",
    "    filtered_tokens.append(tokens)\n",
    "    i += 1\n",
    "    if i % 10000 == 0:\n",
    "        print(f'Tokenized {i} documents in {(time.time()-start)/60} minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the tokenized reviews to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/tokenized.pkl', 'wb') as f:\n",
    "    pickle.dump(filtered_tokens, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = businesses['processed'].astype('unicode').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized 10000 documents in 0.5255980491638184 minutes\n",
      "Tokenized 20000 documents in 1.037276017665863 minutes\n",
      "Tokenized 30000 documents in 1.5686418016751607 minutes\n",
      "Tokenized 40000 documents in 2.1024542530377706 minutes\n",
      "Tokenized 50000 documents in 2.618568793932597 minutes\n",
      "Tokenized 60000 documents in 3.1329532504081725 minutes\n",
      "Tokenized 70000 documents in 3.653790811697642 minutes\n",
      "Tokenized 80000 documents in 4.175454847017924 minutes\n",
      "Tokenized 90000 documents in 4.6936612010002134 minutes\n",
      "Tokenized 100000 documents in 5.195524656772614 minutes\n",
      "Tokenized 110000 documents in 5.699959830443064 minutes\n",
      "Tokenized 120000 documents in 6.218272244930267 minutes\n",
      "Tokenized 130000 documents in 6.734584017594655 minutes\n",
      "Tokenized 140000 documents in 7.238488566875458 minutes\n",
      "Tokenized 150000 documents in 7.749598217010498 minutes\n",
      "Tokenized 160000 documents in 8.263826489448547 minutes\n",
      "Tokenized 170000 documents in 8.79062537352244 minutes\n",
      "Tokenized 180000 documents in 9.313677577177684 minutes\n",
      "Tokenized 190000 documents in 9.848695917924244 minutes\n",
      "Tokenized 200000 documents in 10.365692694981893 minutes\n",
      "Tokenized 210000 documents in 10.88412389755249 minutes\n",
      "Tokenized 220000 documents in 11.419294981161753 minutes\n",
      "Tokenized 230000 documents in 11.931290431817372 minutes\n",
      "Tokenized 240000 documents in 12.443247556686401 minutes\n",
      "Tokenized 250000 documents in 12.96053185860316 minutes\n",
      "Tokenized 260000 documents in 13.481406168142955 minutes\n",
      "Tokenized 270000 documents in 14.006450847784679 minutes\n",
      "Tokenized 280000 documents in 14.514764213562012 minutes\n",
      "Tokenized 290000 documents in 15.03159575064977 minutes\n",
      "Tokenized 300000 documents in 15.553327135245006 minutes\n",
      "Tokenized 310000 documents in 16.07143166859945 minutes\n",
      "Tokenized 320000 documents in 16.599124236901602 minutes\n",
      "Tokenized 330000 documents in 17.125049205621085 minutes\n",
      "Tokenized 340000 documents in 17.630038317044576 minutes\n",
      "Tokenized 350000 documents in 18.148175207773843 minutes\n",
      "Tokenized 360000 documents in 18.68001110156377 minutes\n",
      "Tokenized 370000 documents in 19.203963975111645 minutes\n",
      "Tokenized 380000 documents in 19.707609275976818 minutes\n",
      "Tokenized 390000 documents in 20.211528611183166 minutes\n",
      "Tokenized 400000 documents in 20.729501763979595 minutes\n",
      "Tokenized 410000 documents in 21.24170113801956 minutes\n",
      "Tokenized 420000 documents in 21.74804692665736 minutes\n",
      "Tokenized 430000 documents in 22.27561582326889 minutes\n",
      "Tokenized 440000 documents in 22.788286423683168 minutes\n",
      "Tokenized 450000 documents in 23.325137464205422 minutes\n",
      "Tokenized 460000 documents in 23.852551519870758 minutes\n",
      "Tokenized 470000 documents in 24.364825916290282 minutes\n",
      "Tokenized 480000 documents in 24.89166604280472 minutes\n",
      "Tokenized 490000 documents in 25.409814234574636 minutes\n",
      "Tokenized 500000 documents in 25.91409587065379 minutes\n",
      "Tokenized 510000 documents in 26.430321343739827 minutes\n",
      "Tokenized 520000 documents in 26.962295695145926 minutes\n",
      "Tokenized 530000 documents in 27.479369390010834 minutes\n",
      "Tokenized 540000 documents in 27.996511161327362 minutes\n",
      "Tokenized 550000 documents in 28.52141832113266 minutes\n",
      "Tokenized 560000 documents in 29.038828102747598 minutes\n",
      "Tokenized 570000 documents in 29.52481688261032 minutes\n",
      "Tokenized 580000 documents in 30.022026793162027 minutes\n",
      "Tokenized 590000 documents in 30.514646470546722 minutes\n",
      "Tokenized 600000 documents in 31.015350369612374 minutes\n",
      "Tokenized 610000 documents in 31.532611147562662 minutes\n",
      "Tokenized 620000 documents in 32.03159974813461 minutes\n",
      "Tokenized 630000 documents in 32.51129508415858 minutes\n",
      "Tokenized 640000 documents in 32.99869062900543 minutes\n",
      "Tokenized 650000 documents in 33.47263106107712 minutes\n",
      "Tokenized 660000 documents in 33.953555063406625 minutes\n",
      "Tokenized 670000 documents in 34.44608456691106 minutes\n",
      "Tokenized 680000 documents in 34.93937836488088 minutes\n",
      "Tokenized 690000 documents in 35.42807222207387 minutes\n",
      "Tokenized 700000 documents in 35.91662933031718 minutes\n",
      "Tokenized 710000 documents in 36.40645963350932 minutes\n",
      "Tokenized 720000 documents in 36.8961976925532 minutes\n",
      "Tokenized 730000 documents in 37.39972081184387 minutes\n",
      "Tokenized 740000 documents in 37.8923726439476 minutes\n",
      "Tokenized 750000 documents in 38.39667619466782 minutes\n",
      "Tokenized 760000 documents in 38.879821415742235 minutes\n",
      "Tokenized 770000 documents in 39.36863978306452 minutes\n",
      "Tokenized 780000 documents in 39.86439457734426 minutes\n",
      "Tokenized 790000 documents in 40.37094849745433 minutes\n",
      "Tokenized 800000 documents in 40.85712287425995 minutes\n",
      "Tokenized 810000 documents in 41.373073442777 minutes\n",
      "Tokenized 820000 documents in 41.86966602007548 minutes\n",
      "Tokenized 830000 documents in 42.36687140862147 minutes\n",
      "Tokenized 840000 documents in 42.86789464155833 minutes\n",
      "Tokenized 850000 documents in 43.364374967416126 minutes\n",
      "Tokenized 860000 documents in 43.84239770571391 minutes\n",
      "Tokenized 870000 documents in 44.33144721587499 minutes\n",
      "Tokenized 880000 documents in 44.82750557263692 minutes\n",
      "Tokenized 890000 documents in 45.31948206822077 minutes\n",
      "Tokenized 900000 documents in 45.79662023385366 minutes\n",
      "Tokenized 910000 documents in 46.27282383044561 minutes\n",
      "Tokenized 920000 documents in 46.7730007926623 minutes\n",
      "Tokenized 930000 documents in 47.277682252724965 minutes\n",
      "Tokenized 940000 documents in 47.769803623358406 minutes\n",
      "Tokenized 950000 documents in 48.25805766979853 minutes\n",
      "Tokenized 960000 documents in 48.7510538538297 minutes\n",
      "Tokenized 970000 documents in 49.250197088718416 minutes\n",
      "Tokenized 980000 documents in 49.74201643069585 minutes\n",
      "Tokenized 990000 documents in 50.24760424296061 minutes\n",
      "Tokenized 1000000 documents in 50.73688901265462 minutes\n",
      "Tokenized 1010000 documents in 51.21991997162501 minutes\n",
      "Tokenized 1020000 documents in 51.72560849189758 minutes\n",
      "Tokenized 1030000 documents in 52.217634097735086 minutes\n",
      "Tokenized 1040000 documents in 52.69470007022222 minutes\n",
      "Tokenized 1050000 documents in 53.181259735425314 minutes\n",
      "Tokenized 1060000 documents in 53.697798844178514 minutes\n",
      "Tokenized 1070000 documents in 54.177580841382344 minutes\n",
      "Tokenized 1080000 documents in 54.67545179128647 minutes\n",
      "Tokenized 1090000 documents in 55.160632475217184 minutes\n",
      "Tokenized 1100000 documents in 55.6661302169164 minutes\n",
      "Tokenized 1110000 documents in 56.16386213699977 minutes\n",
      "Tokenized 1120000 documents in 56.65638833045959 minutes\n",
      "Tokenized 1130000 documents in 57.140308086077376 minutes\n",
      "Tokenized 1140000 documents in 57.6330184896787 minutes\n",
      "Tokenized 1150000 documents in 58.10995343526204 minutes\n",
      "Tokenized 1160000 documents in 58.59127031564712 minutes\n",
      "Tokenized 1170000 documents in 59.08732743263245 minutes\n",
      "Tokenized 1180000 documents in 59.567887636025745 minutes\n",
      "Tokenized 1190000 documents in 60.051346667607625 minutes\n",
      "Tokenized 1200000 documents in 60.54120386838913 minutes\n",
      "Tokenized 1210000 documents in 61.03511963685354 minutes\n",
      "Tokenized 1220000 documents in 61.529093472162884 minutes\n",
      "Tokenized 1230000 documents in 62.01737768650055 minutes\n",
      "Tokenized 1240000 documents in 62.5077045083046 minutes\n",
      "Tokenized 1250000 documents in 62.99851922194163 minutes\n",
      "Tokenized 1260000 documents in 63.47842909495036 minutes\n",
      "Tokenized 1270000 documents in 63.970171093940735 minutes\n",
      "Tokenized 1280000 documents in 64.47641698122024 minutes\n",
      "Tokenized 1290000 documents in 64.96342886288961 minutes\n",
      "Tokenized 1300000 documents in 65.45390058755875 minutes\n",
      "Tokenized 1310000 documents in 65.95006172259649 minutes\n",
      "Tokenized 1320000 documents in 66.45225290854772 minutes\n",
      "Tokenized 1330000 documents in 66.94384940862656 minutes\n",
      "Tokenized 1340000 documents in 67.441186718146 minutes\n",
      "Tokenized 1350000 documents in 67.9286367813746 minutes\n",
      "Tokenized 1360000 documents in 68.41541263659795 minutes\n",
      "Tokenized 1370000 documents in 68.94222690264384 minutes\n",
      "Tokenized 1380000 documents in 69.44371971686681 minutes\n",
      "Tokenized 1390000 documents in 69.92979781230291 minutes\n",
      "Tokenized 1400000 documents in 70.41177354256313 minutes\n",
      "Tokenized 1410000 documents in 70.90177170435588 minutes\n",
      "Tokenized 1420000 documents in 71.39858194589615 minutes\n",
      "Tokenized 1430000 documents in 71.88545808792114 minutes\n",
      "Tokenized 1440000 documents in 72.37694650888443 minutes\n",
      "Tokenized 1450000 documents in 72.8661843776703 minutes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized 1460000 documents in 73.36287189324698 minutes\n",
      "Tokenized 1470000 documents in 73.86117143630982 minutes\n",
      "Tokenized 1480000 documents in 74.3657881339391 minutes\n",
      "Tokenized 1490000 documents in 74.8628033320109 minutes\n",
      "Tokenized 1500000 documents in 75.3468784570694 minutes\n",
      "Tokenized 1510000 documents in 75.85534100135168 minutes\n",
      "Tokenized 1520000 documents in 76.3619817574819 minutes\n",
      "Tokenized 1530000 documents in 76.86218363841375 minutes\n",
      "Tokenized 1540000 documents in 77.35893313487371 minutes\n",
      "Tokenized 1550000 documents in 77.84132454792659 minutes\n",
      "Tokenized 1560000 documents in 78.33095343112946 minutes\n",
      "Tokenized 1570000 documents in 78.83524558544158 minutes\n",
      "Tokenized 1580000 documents in 79.33973424832026 minutes\n",
      "Tokenized 1590000 documents in 79.84158979256948 minutes\n",
      "Tokenized 1600000 documents in 80.34113273620605 minutes\n",
      "Tokenized 1610000 documents in 80.83974155982335 minutes\n",
      "Tokenized 1620000 documents in 81.32629689772924 minutes\n",
      "Tokenized 1630000 documents in 81.81672234932581 minutes\n",
      "Tokenized 1640000 documents in 82.30851511160533 minutes\n",
      "Tokenized 1650000 documents in 82.79319481054942 minutes\n",
      "Tokenized 1660000 documents in 83.29581000010172 minutes\n",
      "Tokenized 1670000 documents in 83.79073680241903 minutes\n",
      "Tokenized 1680000 documents in 84.27219947576523 minutes\n",
      "Tokenized 1690000 documents in 84.76192668279012 minutes\n",
      "Tokenized 1700000 documents in 85.26453196605047 minutes\n",
      "Tokenized 1710000 documents in 85.77251564264297 minutes\n",
      "Tokenized 1720000 documents in 86.26390893856684 minutes\n",
      "Tokenized 1730000 documents in 86.75934863885244 minutes\n",
      "Tokenized 1740000 documents in 87.25948957999547 minutes\n",
      "Tokenized 1750000 documents in 87.7494325319926 minutes\n",
      "Tokenized 1760000 documents in 88.28572274446488 minutes\n",
      "Tokenized 1770000 documents in 88.79056479533513 minutes\n",
      "Tokenized 1780000 documents in 89.29108408292134 minutes\n",
      "Tokenized 1790000 documents in 89.77905385891596 minutes\n",
      "Tokenized 1800000 documents in 90.27139616807303 minutes\n",
      "Tokenized 1810000 documents in 90.77810486157735 minutes\n",
      "Tokenized 1820000 documents in 91.27994541327159 minutes\n",
      "Tokenized 1830000 documents in 91.7846870581309 minutes\n",
      "Tokenized 1840000 documents in 92.28262719313304 minutes\n",
      "Tokenized 1850000 documents in 92.77920132478079 minutes\n",
      "Tokenized 1860000 documents in 93.281026160717 minutes\n",
      "Tokenized 1870000 documents in 93.7810696164767 minutes\n",
      "Tokenized 1880000 documents in 94.26923667192459 minutes\n",
      "Tokenized 1890000 documents in 94.75621133645376 minutes\n",
      "Tokenized 1900000 documents in 95.25420500040055 minutes\n",
      "Tokenized 1910000 documents in 95.76229631106058 minutes\n",
      "Tokenized 1920000 documents in 96.25764964024226 minutes\n",
      "Tokenized 1930000 documents in 96.76412864526112 minutes\n",
      "Tokenized 1940000 documents in 97.2591675877571 minutes\n",
      "Tokenized 1950000 documents in 97.7723125298818 minutes\n",
      "Tokenized 1960000 documents in 98.27025154034297 minutes\n",
      "Tokenized 1970000 documents in 98.79108395179112 minutes\n",
      "Tokenized 1980000 documents in 99.28327909708023 minutes\n",
      "Tokenized 1990000 documents in 99.76577100753784 minutes\n",
      "Tokenized 2000000 documents in 100.26309564113618 minutes\n",
      "Tokenized 2010000 documents in 100.76504382689794 minutes\n",
      "Tokenized 2020000 documents in 101.25108903249105 minutes\n",
      "Tokenized 2030000 documents in 101.74932932456335 minutes\n",
      "Tokenized 2040000 documents in 102.23829953670501 minutes\n"
     ]
    }
   ],
   "source": [
    "filtered_tokens_bs = []\n",
    "start = time.time()\n",
    "i = 1\n",
    "for doc in nlp.pipe(docs, disable=['tagger', 'parser', 'ner', 'textcat'], batch_size=10000):\n",
    "    tokens = [token.lemma_ for token in doc if token_filter(token)]\n",
    "    filtered_tokens_bs.append(tokens)\n",
    "    i += 1\n",
    "    if i % 10000 == 0:\n",
    "        print(f'Tokenized {i} documents in {(time.time()-start)/60} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/tokenized_bs.pkl', 'wb') as f:\n",
    "    pickle.dump(filtered_tokens_bs, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
