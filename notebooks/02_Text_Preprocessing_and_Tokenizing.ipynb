{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing and Tokenizing\n",
    "\n",
    "In this notebook, we process and tokenize each review text. In the preprocessing stage, the text is converted to lowercase and words are lemmatized. In the tokenizing stage, each review is converted into a document that contains single words as tokens. These documents are then compiled into a corpus.\n",
    "\n",
    "Since restaurants dominate the Yelp review space, restaurant reviews and reviews for other businesses will be assessed separately.\n",
    "\n",
    "## Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "import textacy\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "### Restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rests = pd.read_csv('../data/restaurants.csv', compression='gzip', usecols=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3055990, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rests.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query for Other Businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "businesses = pd.read_csv('../data/businesses.csv', compression='gzip', usecols=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1968973, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "businesses.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the text\n",
    "\n",
    "Using textacy's preprocess method, I converted all review text to lowercase and remove numbers, URLs, and punctuation. The textacy preprocessor will convert numbers to the string 'numb' and URLs to the string 'URL'. I chose to combine numbers because time and price descriptions are likely to be very common in reviews. Rather than having individual tokens for '5 minutes' and '10 dollars', documents processed in this way will contain a 'numb' token whenever a numeric term is encountered. \n",
    "\n",
    "### Restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23min 16s, sys: 1.88 s, total: 23min 18s\n",
      "Wall time: 23min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rests['processed'] = rests['text'].map(lambda x: textacy.preprocess.preprocess_text(x, lowercase=True, \n",
    "                                                                                    no_urls=True, \n",
    "                                                                                    no_punct=True, \n",
    "                                                                                    no_numbers=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15min 17s, sys: 1.24 s, total: 15min 18s\n",
      "Wall time: 15min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "businesses['processed'] = businesses['text'].map(lambda x: textacy.preprocess.preprocess_text(x, lowercase=True, \n",
    "                                                                                              no_urls=True,\n",
    "                                                                                              no_punct=True, \n",
    "                                                                                              no_numbers=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for non-English reviews\n",
    "\n",
    "Since Yelp is used worldwide, I checked for reviews containing non-English characters as non-English words or statements may not be tokenized or processed correctly. I define the function `isEnglish` to filter out non-ASCII characters as non-ASCII characters would predominantly be used by users typing reviews in non-English languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define `isEnglish` function\n",
    "\n",
    "Strings that contain non-English characters will return a UnicodeEncodeError when attempting to convert them to the ASCII format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode('ascii')\n",
    "    except UnicodeEncodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rests['isEnglish'] = rests['processed'].astype('str').astype('unicode').apply(lambda x: isEnglish(x) == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     2984421\n",
       "False      71569\n",
       "Name: isEnglish, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rests['isEnglish'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rests = rests[rests['isEnglish'] == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I save the index of the reviews as an array to retain for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../data/rests_eng_index.npy', rests[rests['isEnglish'] == True].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "businesses['isEnglish'] = businesses['processed'].astype('str').astype('unicode').apply(lambda x: isEnglish(x) == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     1952542\n",
       "False      16431\n",
       "Name: isEnglish, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "businesses['isEnglish'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../data/bus_eng_index.npy', businesses[businesses['isEnglish'] == True].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing\n",
    "\n",
    "In this section I set up a spacy tokenizer. We disable part-of-speech tagging, semantic parsing, and text categorization to reduce overall memory usage, and choose to retain the lemmas of each token. We also create a filter function to eliminate stopwords and short tokens (less than 4 characters). The tokenized documents are then added to a list which we can pass through a vectorizer (see Notebook 3.)\n",
    "\n",
    "**The tokenizing loops below have had their output cleared to improve this notebook's readability.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = textacy.load_spacy(\"en_core_web_sm\", disable = (\"tagger\", \"parser\", \"ner\", \"textcat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_filter(token): \n",
    "    return not (token.is_stop | len(token.text) <= 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = rests['processed'].astype('str').astype('unicode').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filtered_tokens = []\n",
    "start = time.time()\n",
    "i = 1\n",
    "for doc in nlp.pipe(docs, disable=['tagger', 'parser', 'ner', 'textcat'], batch_size=10000):\n",
    "    try:\n",
    "        tokens = [token.lemma_ for token in doc if token_filter(token)]\n",
    "        filtered_tokens.append(tokens)\n",
    "        i += 1\n",
    "        if i % 10000 == 0:\n",
    "            print(f'Tokenized {i} documents in {(time.time()-start)/60} minutes')\n",
    "    except:\n",
    "        print(f'Document {i} has an encoding error/has error characters.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell takes around 3 hours to tokenize approximately 3 million reviews and append them to the list `filtered_tokens`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the tokenized restaurant reviews to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/tokenized_rest_reviews.pkl', 'wb') as f:\n",
    "    pickle.dump(filtered_tokens, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = businesses['processed'].astype('str').astype('unicode').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filtered_tokens_bs = []\n",
    "start = time.time()\n",
    "i = 1\n",
    "for doc in nlp.pipe(docs, disable=['tagger', 'parser', 'ner', 'textcat'], batch_size=10000):\n",
    "    try:\n",
    "        tokens = [token.lemma_ for token in doc if token_filter(token)]\n",
    "        filtered_tokens_bs.append(tokens)\n",
    "        i += 1\n",
    "        if i % 10000 == 0:\n",
    "            print(f'Tokenized {i} documents in {(time.time()-start)/60} minutes')\n",
    "    except:\n",
    "        print(f'Document {i} has an encoding error.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell takes around 2 hours to tokenize approximately 2 million reviews. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the tokenized business reviews to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/tokenized_bs_reviews.pkl', 'wb') as f:\n",
    "    pickle.dump(filtered_tokens_bs, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
